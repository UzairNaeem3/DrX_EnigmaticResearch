{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYtvklBPvNiM",
    "outputId": "ce45669e-8f91-49b3-ea4b-b9209ff7bac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m745.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install torch fasttext python-docx PyPDF2 beautifulsoup4 reportlab bitsandbytes -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kkmyUXDABMQ",
    "outputId": "0c1d2f43-0a50-4bd8-9fa1-7d6567d350fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n",
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
      "peft 0.14.0 requires transformers, which is not installed.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m217.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy transformers\n",
    "!pip install numpy==1.26.4\n",
    "!pip install transformers --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q33sJC_MbyXR"
   },
   "source": [
    "After installing the necessary libraries, it's important to restart the session. This ensures that the newly installed packages are properly loaded into the environment. Without restarting, the current session may not recognize the changes, and the libraries might not work as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1uHmvGswjd3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUERLkqxgGiu"
   },
   "source": [
    "### **Decorator to measure tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RB6nwo-Icax"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "def measure_token_processing(process_name: Optional[str] = None):\n",
    "    \"\"\"Decorator to measure token processing speed across different tasks.\"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Determine the process name\n",
    "            task_name = process_name or func.__name__\n",
    "\n",
    "            # Get the instance (self) from args\n",
    "            instance = args[0] if args else None\n",
    "\n",
    "            # Find text to tokenize - could be in different places depending on function\n",
    "            text = None\n",
    "            # Check args - likely the second argument for most methods\n",
    "            if len(args) > 1 and isinstance(args[1], str):\n",
    "                text = args[1]\n",
    "            elif len(args) > 1 and isinstance(args[1], list) and all(isinstance(t, str) for t in args[1]):\n",
    "                text = ' '.join(args[1])  # Join list of strings\n",
    "\n",
    "            # Check kwargs for text\n",
    "            elif 'text' in kwargs and isinstance(kwargs['text'], str):\n",
    "                text = kwargs['text']\n",
    "            elif 'texts' in kwargs and isinstance(kwargs['texts'], list):\n",
    "                text = ' '.join(kwargs['texts'])\n",
    "            elif 'prompt' in kwargs and isinstance(kwargs['prompt'], str):\n",
    "                text = kwargs['prompt']\n",
    "\n",
    "            # Find appropriate tokenizer\n",
    "            tokenizer = None\n",
    "            if instance and hasattr(instance, 'tokenizer'):\n",
    "                tokenizer = instance.tokenizer\n",
    "            elif instance and hasattr(instance, 'embedding_model') and hasattr(instance.embedding_model, 'tokenizer'):\n",
    "                tokenizer = instance.embedding_model.tokenizer\n",
    "\n",
    "            # Count tokens if we have both text and tokenizer\n",
    "            input_tokens = 0\n",
    "            if text and tokenizer:\n",
    "                try:\n",
    "                    if hasattr(tokenizer, 'encode'):\n",
    "                        input_tokens = len(tokenizer.encode(text))\n",
    "                    elif hasattr(tokenizer, '__call__'):\n",
    "                        input_tokens = len(tokenizer(text)['input_ids'])\n",
    "                except:\n",
    "                    # Fallback to rough estimate (approx 4 chars per token)\n",
    "                    input_tokens = len(text) // 4\n",
    "            elif text:\n",
    "                # Very rough approximation if no tokenizer available\n",
    "                input_tokens = len(text.split())\n",
    "\n",
    "            # Start timing\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Execute the function\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            # End timing\n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            processing_time = end_time - start_time\n",
    "            tokens_per_second = input_tokens / processing_time if processing_time > 0 else 0\n",
    "\n",
    "            # Output measurements\n",
    "            print(f\"⏱️ {task_name}: {input_tokens} tokens processed in {processing_time:.2f}s ({tokens_per_second:.2f} tokens/sec)\\n\")\n",
    "\n",
    "            # If result is a string, we could measure output tokens too\n",
    "            if isinstance(result, str) and tokenizer:\n",
    "                try:\n",
    "                    output_tokens = len(tokenizer.encode(result))\n",
    "                    total_tokens = input_tokens + output_tokens\n",
    "                    throughput = total_tokens / processing_time if processing_time > 0 else 0\n",
    "                    print(f\"   Total (in+out): {total_tokens} tokens at {throughput:.2f} tokens/sec\\n\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFeKtBpPQgkC"
   },
   "source": [
    "### **Translation Service**\n",
    "\n",
    "A simple yet effective document translation system that uses local LLMs:\n",
    "\n",
    "- **Local Language Model Support**: Uses lightweight models like NLLB and M2M100 for translation\n",
    "- **Format Preservation**: Maintains document structure, paragraphs, lists, and tables during translation\n",
    "- **Chunking Strategy**: Intelligently splits text into manageable pieces while preserving semantic integrity\n",
    "- **Multi-Document Support**: Handles various formats including PDF, DOCX, and plain text\n",
    "- **Language Auto-Detection**: Automatically identifies source language when not specified\n",
    "- **Resource Optimization**: Implements model caching and fallback to smaller models when needed\n",
    "- **Table and Code Handling**: Preserves special content formats that shouldn't be fully translated\n",
    "\n",
    "This service provides efficient translation capabilities that can run entirely on local hardware without requiring external API calls or cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stq9amsdgi2X"
   },
   "outputs": [],
   "source": [
    "# Login to HF to access LLMs\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft3DHv_Q0SsA"
   },
   "outputs": [],
   "source": [
    "class TranslationService:\n",
    "    \"\"\"\n",
    "    A service for translating documents between languages using local LLMs.\n",
    "    Focuses on maintaining document structure and formatting during translation.\n",
    "    \"\"\"\n",
    "    # Class-level cache for loaded models\n",
    "    _loaded_translation_model = None\n",
    "    _loaded_translation_model_name = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"facebook/nllb-200-distilled-600M\",  # Smaller NLLB model suitable for local use\n",
    "        device: str = \"auto\",\n",
    "        preserve_formatting: bool = True\n",
    "    ):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.preserve_formatting = preserve_formatting\n",
    "\n",
    "        # Determine device\n",
    "        if device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        print(f\"-----Initializing translation service with model {model_name} on {self.device}-----\\n\")\n",
    "\n",
    "        # Load translation model (with caching)\n",
    "        self._load_model()\n",
    "\n",
    "        # Map of language codes for NLLB\n",
    "        self.language_code_map = self._initialize_language_codes()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the translation model and tokenizer, with caching.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Check if the model is already loaded\n",
    "            if (TranslationService._loaded_translation_model is not None and\n",
    "                TranslationService._loaded_translation_model_name == self.model_name):\n",
    "                print(\"Translation model already loaded. Reusing from cache.\")\n",
    "                self.model = TranslationService._loaded_translation_model\n",
    "                self.tokenizer = self.model.tokenizer\n",
    "                return\n",
    "\n",
    "            # Import necessary libraries\n",
    "            from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "            print(f\"-----Loading translation model: {self.model_name}-----\\n\")\n",
    "\n",
    "            # Load the tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "            # Load model with appropriate settings based on device\n",
    "            if self.device == \"cuda\":\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    load_in_8bit=True\n",
    "                )\n",
    "            else:\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map={\"\": self.device}\n",
    "                )\n",
    "\n",
    "            # Cache the loaded model\n",
    "            TranslationService._loaded_translation_model = self.model\n",
    "            TranslationService._loaded_translation_model_name = self.model_name\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading translation model: {str(e)}\")\n",
    "            print(\"-----Attempting to load a smaller fallback model...-----\\n\")\n",
    "\n",
    "            try:\n",
    "                # Fallback to an even smaller model\n",
    "                fallback_model = \"facebook/m2m100_418M\"\n",
    "                self.model_name = fallback_model\n",
    "\n",
    "                from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "                self.tokenizer = M2M100Tokenizer.from_pretrained(fallback_model)\n",
    "                self.model = M2M100ForConditionalGeneration.from_pretrained(\n",
    "                    fallback_model,\n",
    "                    device_map={\"\": self.device}\n",
    "                )\n",
    "\n",
    "                # Cache the fallback model\n",
    "                TranslationService._loaded_translation_model = self.model\n",
    "                TranslationService._loaded_translation_model_name = fallback_model\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"Error loading fallback model: {str(e2)}\")\n",
    "                print(\"-----Translation service will not be functional.-----\\n\")\n",
    "                self.model = None\n",
    "                self.tokenizer = None\n",
    "\n",
    "    def _initialize_language_codes(self):\n",
    "        \"\"\"Initialize the mapping of language names to NLLB language codes.\"\"\"\n",
    "\n",
    "        # This is a partial list of NLLB language codes - expand as needed\n",
    "        return {\n",
    "            \"english\": \"eng_Latn\",\n",
    "            \"arabic\": \"arb_Arab\",\n",
    "            \"french\": \"fra_Latn\",\n",
    "            \"spanish\": \"spa_Latn\",\n",
    "            \"german\": \"deu_Latn\",\n",
    "            \"chinese\": \"zho_Hans\",\n",
    "            \"japanese\": \"jpn_Jpan\",\n",
    "            \"russian\": \"rus_Cyrl\",\n",
    "            \"portuguese\": \"por_Latn\",\n",
    "            \"italian\": \"ita_Latn\",\n",
    "            \"dutch\": \"nld_Latn\",\n",
    "            \"swedish\": \"swe_Latn\",\n",
    "            \"korean\": \"kor_Hang\",\n",
    "            \"turkish\": \"tur_Latn\",\n",
    "            \"hindi\": \"hin_Deva\",\n",
    "            \"bengali\": \"ben_Beng\",\n",
    "            \"urdu\": \"urd_Arab\",\n",
    "            \"persian\": \"fas_Arab\",\n",
    "            \"thai\": \"tha_Thai\",\n",
    "            \"vietnamese\": \"vie_Latn\",\n",
    "            \"indonesian\": \"ind_Latn\",\n",
    "            \"malay\": \"zsm_Latn\",\n",
    "            \"polish\": \"pol_Latn\",\n",
    "            \"ukrainian\": \"ukr_Cyrl\",\n",
    "            \"greek\": \"ell_Grek\",\n",
    "            \"czech\": \"ces_Latn\",\n",
    "            \"finnish\": \"fin_Latn\",\n",
    "            \"hungarian\": \"hun_Latn\"\n",
    "        }\n",
    "\n",
    "    def get_language_code(self, language: str) -> str:\n",
    "        \"\"\"Get the NLLB language code for a given language name.\"\"\"\n",
    "\n",
    "        language = language.lower().strip()\n",
    "        if language in self.language_code_map:\n",
    "            return self.language_code_map[language]\n",
    "        else:\n",
    "            # If language not found, attempt to find partial matches\n",
    "            for lang, code in self.language_code_map.items():\n",
    "                if language in lang or lang in language:\n",
    "                    return code\n",
    "\n",
    "            # Default to English if no match found\n",
    "            print(f\"Warning: Language '{language}' not found in mapping. Using English as default.\\n\")\n",
    "            return self.language_code_map[\"english\"]\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of the text.\"\"\"\n",
    "        try:\n",
    "            # Import fasttext for language detection\n",
    "            import fasttext\n",
    "\n",
    "            # Check if the model exists and download if needed\n",
    "            model_path = \"lid.176.bin\"\n",
    "            if not os.path.exists(model_path):\n",
    "                print(\"-----Downloading fastText language detection model...-----\\n\")\n",
    "                import urllib.request\n",
    "                url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "                urllib.request.urlretrieve(url, model_path)\n",
    "\n",
    "            # Load the model\n",
    "            model = fasttext.load_model(model_path)\n",
    "\n",
    "            # Detect language\n",
    "            prediction = model.predict(text.replace(\"\\n\", \" \"))\n",
    "\n",
    "            # Extract language code and map to NLLB code\n",
    "            detected_lang = prediction[0][0].replace(\"__label__\", \"\")\n",
    "\n",
    "            # Map ISO code to our language name\n",
    "            iso_to_name = {\n",
    "                \"en\": \"english\",\n",
    "                \"ar\": \"arabic\",\n",
    "                \"fr\": \"french\",\n",
    "                \"es\": \"spanish\",\n",
    "                \"de\": \"german\",\n",
    "                \"zh\": \"chinese\",\n",
    "                \"ja\": \"japanese\",\n",
    "                \"ru\": \"russian\",\n",
    "                \"pt\": \"portuguese\",\n",
    "                \"it\": \"italian\",\n",
    "                \"nl\": \"dutch\",\n",
    "                \"sv\": \"swedish\",\n",
    "                \"ko\": \"korean\",\n",
    "                \"tr\": \"turkish\",\n",
    "                \"hi\": \"hindi\",\n",
    "                \"bn\": \"bengali\",\n",
    "                \"ur\": \"urdu\",\n",
    "                \"fa\": \"persian\",\n",
    "                \"th\": \"thai\",\n",
    "                \"vi\": \"vietnamese\",\n",
    "                \"id\": \"indonesian\",\n",
    "                \"ms\": \"malay\",\n",
    "                \"pl\": \"polish\",\n",
    "                \"uk\": \"ukrainian\",\n",
    "                \"el\": \"greek\",\n",
    "                \"cs\": \"czech\",\n",
    "                \"fi\": \"finnish\",\n",
    "                \"hu\": \"hungarian\"\n",
    "            }\n",
    "\n",
    "            if detected_lang in iso_to_name:\n",
    "                return self.get_language_code(iso_to_name[detected_lang])\n",
    "            else:\n",
    "                print(f\"Warning: Detected language '{detected_lang}' not mapped to NLLB code. Using English.\\n\")\n",
    "                return self.language_code_map[\"english\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in language detection: {str(e)}\")\n",
    "            print(\"Falling back to English\")\n",
    "            return self.language_code_map[\"english\"]\n",
    "\n",
    "    def _split_into_chunks(self, text: str, max_length: int = 512) -> list:\n",
    "        \"\"\"Split text into chunks for translation, preserving structure.\"\"\"\n",
    "\n",
    "        # If text is shorter than max_length, return it as a single chunk\n",
    "        if len(self.tokenizer.encode(text)) <= max_length:\n",
    "            return [text]\n",
    "\n",
    "        # First, split by paragraphs\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # If paragraph is too long, split it by sentences\n",
    "            if len(self.tokenizer.encode(paragraph)) > max_length:\n",
    "                sentences = self._split_into_sentences(paragraph)\n",
    "                for sentence in sentences:\n",
    "                    # If adding this sentence would make the chunk too long, start a new chunk\n",
    "                    if len(self.tokenizer.encode(current_chunk + sentence)) > max_length and current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = sentence\n",
    "                    else:\n",
    "                        current_chunk += sentence\n",
    "            else:\n",
    "                # If adding this paragraph would make the chunk too long, start a new chunk\n",
    "                if len(self.tokenizer.encode(current_chunk + paragraph + \"\\n\\n\")) > max_length and current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = paragraph + \"\\n\\n\"\n",
    "                else:\n",
    "                    current_chunk += paragraph + \"\\n\\n\"\n",
    "\n",
    "        # Add the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"Split text into sentences.\"\"\"\n",
    "\n",
    "        # Simple sentence splitting - enhance this for better results\n",
    "        import re\n",
    "        sentence_endings = r'(?<=[.!?])\\s+'\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        return [s + \". \" if not s.endswith(('.', '!', '?')) else s + \" \" for s in sentences if s]\n",
    "\n",
    "    def _preserve_formatting(self, original: str, translated: str) -> str:\n",
    "        \"\"\"Attempt to preserve formatting from original text in the translated text.\"\"\"\n",
    "\n",
    "        if not self.preserve_formatting:\n",
    "            return translated\n",
    "\n",
    "        # Preserve paragraph breaks\n",
    "        if \"\\n\\n\" in original and \"\\n\\n\" not in translated:\n",
    "            # Count paragraphs in original\n",
    "            orig_paragraphs = original.split(\"\\n\\n\")\n",
    "            if len(orig_paragraphs) > 1:\n",
    "                # Try to split translated text into similar number of paragraphs\n",
    "                trans_sentences = self._split_into_sentences(translated)\n",
    "                sentences_per_paragraph = max(1, len(trans_sentences) // len(orig_paragraphs))\n",
    "\n",
    "                new_translated = \"\"\n",
    "                for i in range(0, len(trans_sentences), sentences_per_paragraph):\n",
    "                    paragraph = \"\".join(trans_sentences[i:i+sentences_per_paragraph])\n",
    "                    new_translated += paragraph.strip() + \"\\n\\n\"\n",
    "\n",
    "                translated = new_translated.strip()\n",
    "\n",
    "        # Preserve bullet points and numbering\n",
    "        bullet_pattern = r'^\\s*[-•*]\\s+'\n",
    "        number_pattern = r'^\\s*\\d+[.)]?\\s+'\n",
    "\n",
    "        if re.search(bullet_pattern, original, re.MULTILINE) and not re.search(bullet_pattern, translated, re.MULTILINE):\n",
    "            # Original has bullet points but translated doesn't\n",
    "            orig_lines = original.split('\\n')\n",
    "            trans_lines = translated.split('\\n')\n",
    "\n",
    "            if len(orig_lines) == len(trans_lines):\n",
    "                # Match line by line\n",
    "                for i in range(len(orig_lines)):\n",
    "                    if re.match(bullet_pattern, orig_lines[i]):\n",
    "                        bullet = re.match(bullet_pattern, orig_lines[i]).group(0)\n",
    "                        trans_lines[i] = bullet + trans_lines[i].lstrip()\n",
    "\n",
    "                translated = '\\n'.join(trans_lines)\n",
    "\n",
    "        # Preserve tables (simple implementation)\n",
    "        if '|' in original and '|' not in translated:\n",
    "            # Try to identify table structures and preserve them\n",
    "            table_rows = re.findall(r'^\\s*\\|.*\\|\\s*$', original, re.MULTILINE)\n",
    "            if table_rows:\n",
    "                # Extract table content and structure\n",
    "                table_content = {}\n",
    "                for i, row in enumerate(table_rows):\n",
    "                    cells = [cell.strip() for cell in row.split('|')[1:-1]]\n",
    "                    table_content[i] = cells\n",
    "\n",
    "                # Translate table content while preserving structure\n",
    "                translated_table = {}\n",
    "                for row_idx, cells in table_content.items():\n",
    "                    translated_cells = []\n",
    "                    for cell in cells:\n",
    "                        if cell and not all(c in '- :' for c in cell):  # Skip separator rows\n",
    "                            translated_cell = self.translate(cell, target_language=\"current\")\n",
    "                            translated_cells.append(translated_cell)\n",
    "                        else:\n",
    "                            translated_cells.append(cell)\n",
    "                    translated_table[row_idx] = translated_cells\n",
    "\n",
    "                # Replace original table with translated one\n",
    "                for i, row in enumerate(table_rows):\n",
    "                    if i in translated_table:\n",
    "                        new_row = '| ' + ' | '.join(translated_table[i]) + ' |'\n",
    "                        translated = translated.replace(row, new_row)\n",
    "\n",
    "        # Preserve code blocks\n",
    "        code_block_pattern = r'```(?:\\w+)?\\n[\\s\\S]*?\\n```'\n",
    "        code_blocks = re.findall(code_block_pattern, original)\n",
    "        if code_blocks:\n",
    "            for code_block in code_blocks:\n",
    "                # Don't translate code blocks, keep them as is\n",
    "                if code_block in original and code_block not in translated:\n",
    "                    translated = translated + \"\\n\\n\" + code_block\n",
    "\n",
    "        return translated\n",
    "\n",
    "    @measure_token_processing(\"Translation\")\n",
    "    def translate(self, text: str, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate text to the target language.\"\"\"\n",
    "\n",
    "        if not text.strip():\n",
    "            return text\n",
    "\n",
    "        if self.model is None:\n",
    "            return \"Translation model not loaded properly.\"\n",
    "\n",
    "        try:\n",
    "            # Special case for \"current\" language target - just return original text\n",
    "            if target_language.lower() == \"current\":\n",
    "                return text\n",
    "\n",
    "            # Get language codes\n",
    "            target_lang_code = self.get_language_code(target_language)\n",
    "\n",
    "            # Auto-detect source language if not provided\n",
    "            if not source_language:\n",
    "                source_lang_code = self.detect_language(text)\n",
    "                print(f\"-----Detected source language code: {source_lang_code}-----\\n\")\n",
    "            else:\n",
    "                source_lang_code = self.get_language_code(source_language)\n",
    "\n",
    "            # If source and target languages are the same, return original text\n",
    "            if source_lang_code == target_lang_code:\n",
    "                return text\n",
    "\n",
    "            # Split text into manageable chunks\n",
    "            chunks = self._split_into_chunks(text)\n",
    "            translated_chunks = []\n",
    "\n",
    "            # Translate each chunk\n",
    "            for chunk in chunks:\n",
    "                if not chunk.strip():\n",
    "                    translated_chunks.append(chunk)\n",
    "                    continue\n",
    "\n",
    "                # Prepare inputs for the model\n",
    "                if \"nllb\" in self.model_name.lower():\n",
    "                    # NLLB model\n",
    "                    self.tokenizer.src_lang = source_lang_code\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        generated_tokens = self.model.generate(\n",
    "                            **inputs,\n",
    "                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(target_lang_code),\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.batch_decode(\n",
    "                        generated_tokens, skip_special_tokens=True\n",
    "                    )[0]\n",
    "\n",
    "                elif \"m2m100\" in self.model_name.lower():\n",
    "                    # M2M100 model\n",
    "                    self.tokenizer.src_lang = source_lang_code.split('_')[0]\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                    self.tokenizer.tgt_lang = target_lang_code.split('_')[0]\n",
    "                    with torch.no_grad():\n",
    "                        generated_tokens = self.model.generate(\n",
    "                            **inputs,\n",
    "                            forced_bos_token_id=self.tokenizer.get_lang_id(target_lang_code.split('_')[0]),\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.batch_decode(\n",
    "                        generated_tokens, skip_special_tokens=True\n",
    "                    )[0]\n",
    "\n",
    "                else:\n",
    "                    # Generic approach for other models\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model.generate(\n",
    "                            **inputs,\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "                # Preserve formatting\n",
    "                if self.preserve_formatting:\n",
    "                    translated_chunk = self._preserve_formatting(chunk, translated_chunk)\n",
    "\n",
    "                translated_chunks.append(translated_chunk)\n",
    "\n",
    "            # Combine translated chunks\n",
    "            full_translation = \" \".join(translated_chunks)\n",
    "\n",
    "            # Clean up any artifacts from the chunking process\n",
    "            full_translation = full_translation.replace(\"  \", \" \")\n",
    "            full_translation = re.sub(r'\\s+\\.', '.', full_translation)\n",
    "            full_translation = re.sub(r'\\s+,', ',', full_translation)\n",
    "\n",
    "            return full_translation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in translation: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return f\"Translation error: {str(e)}\"\n",
    "\n",
    "    def translate_file(self, file_path: str, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate a file to the target language.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Determine file type and handle accordingly\n",
    "            file_path = Path(file_path)\n",
    "            file_extension = file_path.suffix.lower()\n",
    "\n",
    "            # Create output file path\n",
    "            output_dir = file_path.parent / \"translated\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            output_file = output_dir / f\"{file_path.stem}_{target_language}{file_extension}\"\n",
    "\n",
    "            if file_extension in ['.docx', '.doc']:\n",
    "                # Word document - use python-docx\n",
    "                return self._translate_docx(file_path, output_file, target_language, source_language)\n",
    "\n",
    "            elif file_extension == '.pdf':\n",
    "                # PDF - extract text, translate, create new PDF\n",
    "                return self._translate_pdf(file_path, output_file, target_language, source_language)\n",
    "\n",
    "            else:\n",
    "                # Default to treating as plain text\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                translated_content = self.translate(content, target_language, source_language)\n",
    "\n",
    "            # Write translated content to output file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(translated_content)\n",
    "\n",
    "            return str(output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating file: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return f\"File translation error: {str(e)}\"\n",
    "\n",
    "    def _translate_json(self, data, target_language: str, source_language: str = None):\n",
    "        \"\"\"Recursively translate values in a JSON object.\"\"\"\n",
    "\n",
    "        if isinstance(data, str):\n",
    "            return self.translate(data, target_language, source_language)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._translate_json(item, target_language, source_language) for item in data]\n",
    "        elif isinstance(data, dict):\n",
    "            return {k: self._translate_json(v, target_language, source_language) for k, v in data.items()}\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "\n",
    "    def _translate_docx(self, input_path, output_path, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate a Word document.\"\"\"\n",
    "\n",
    "        try:\n",
    "            import docx\n",
    "\n",
    "            # Load the document\n",
    "            doc = docx.Document(input_path)\n",
    "\n",
    "            # Translate each paragraph\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    translated_text = self.translate(para.text, target_language, source_language)\n",
    "\n",
    "                    # Preserve runs with their formatting\n",
    "                    if len(para.runs) > 1:\n",
    "                        # This is an approximation and won't be perfect\n",
    "                        words = translated_text.split()\n",
    "                        runs_text = [run.text for run in para.runs if run.text.strip()]\n",
    "\n",
    "                        # Calculate distribution\n",
    "                        total_original_length = sum(len(text) for text in runs_text)\n",
    "                        words_per_run = []\n",
    "\n",
    "                        for run_text in runs_text:\n",
    "                            proportion = len(run_text) / total_original_length if total_original_length > 0 else 0\n",
    "                            words_for_run = max(1, int(proportion * len(words)))\n",
    "                            words_per_run.append(words_for_run)\n",
    "\n",
    "                        # Adjust to make sure all words are distributed\n",
    "                        while sum(words_per_run) < len(words):\n",
    "                            words_per_run[-1] += 1\n",
    "\n",
    "                        # Apply to runs\n",
    "                        word_index = 0\n",
    "                        for i, run in enumerate(para.runs):\n",
    "                            if i < len(words_per_run):\n",
    "                                run_words = words[word_index:word_index + words_per_run[i]]\n",
    "                                run.text = \" \".join(run_words)\n",
    "                                word_index += words_per_run[i]\n",
    "                            else:\n",
    "                                run.text = \"\"\n",
    "                    else:\n",
    "                        # Simple case - just replace text\n",
    "                        para.text = translated_text\n",
    "\n",
    "            # Translate table contents\n",
    "            for table in doc.tables:\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        for para in cell.paragraphs:\n",
    "                            if para.text.strip():\n",
    "                                para.text = self.translate(para.text, target_language, source_language)\n",
    "\n",
    "            # Save the document\n",
    "            doc.save(output_path)\n",
    "\n",
    "            return str(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating Word document: {str(e)}\")\n",
    "            return f\"Word document translation error: {str(e)}\"\n",
    "\n",
    "\n",
    "    def _translate_pdf(self, input_path, output_path, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from PDF, translate it page by page, and save only as text files.\n",
    "        Returns the path to the complete translated text file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import PyPDF2\n",
    "            import os\n",
    "            from pathlib import Path\n",
    "\n",
    "            # Convert Path objects to strings\n",
    "            input_path_str = str(input_path)\n",
    "\n",
    "            # Create output directory and path for text file\n",
    "            output_dir = os.path.dirname(str(output_path))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Create text file path (replace .pdf with .txt if needed)\n",
    "            output_txt = str(output_path).replace('.pdf', '.txt')\n",
    "\n",
    "            # Create directory for individual pages\n",
    "            base_filename = os.path.basename(output_txt).split('.')[0]\n",
    "            pages_dir = os.path.join(output_dir, f\"{base_filename}_pages\")\n",
    "            os.makedirs(pages_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Starting page-by-page translation of PDF: {input_path_str}\")\n",
    "\n",
    "            # Open the PDF\n",
    "            with open(input_path_str, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                total_pages = len(reader.pages)\n",
    "                print(f\"PDF has {total_pages} pages\")\n",
    "\n",
    "                all_translated_text = \"\"\n",
    "\n",
    "                # Process each page individually\n",
    "                for page_num in range(total_pages):\n",
    "                    try:\n",
    "                        print(f\"Processing page {page_num + 1} of {total_pages}\")\n",
    "\n",
    "                        # Extract text from this page\n",
    "                        page_text = reader.pages[page_num].extract_text()\n",
    "\n",
    "                        if not page_text.strip():\n",
    "                            print(f\"Page {page_num + 1} appears to be empty or contains only images\")\n",
    "                            page_info = f\"\\n\\n--- Page {page_num + 1} (Empty or contains only images) ---\\n\\n\"\n",
    "                            all_translated_text += page_info\n",
    "\n",
    "                            # Save empty page file as a placeholder\n",
    "                            page_file = os.path.join(pages_dir, f\"page_{page_num + 1}.txt\")\n",
    "                            with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                                f.write(page_info)\n",
    "                            continue\n",
    "\n",
    "                        # Translate this page's text\n",
    "                        translated_page = self.translate(page_text, target_language, source_language)\n",
    "\n",
    "                        # Add page information\n",
    "                        page_content = f\"\\n\\n--- Page {page_num + 1} ---\\n\\n{translated_page}\"\n",
    "                        all_translated_text += page_content\n",
    "\n",
    "                        # Save individual page translation\n",
    "                        page_file = os.path.join(pages_dir, f\"page_{page_num + 1}.txt\")\n",
    "                        with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(page_content)\n",
    "\n",
    "                        print(f\"Saved translated page {page_num + 1} to {page_file}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing page {page_num + 1}: {str(e)}\")\n",
    "                        error_message = f\"\\n\\n--- Error on Page {page_num + 1}: {str(e)} ---\\n\\n\"\n",
    "                        all_translated_text += error_message\n",
    "\n",
    "                        # Save error information\n",
    "                        page_file = os.path.join(pages_dir, f\"page_{page_num + 1}_error.txt\")\n",
    "                        with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(error_message)\n",
    "\n",
    "            # Save the complete text version\n",
    "            with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "                f.write(all_translated_text)\n",
    "            print(f\"Saved complete translated text to {output_txt}\")\n",
    "            print(f\"Individual translated pages saved to {pages_dir}\")\n",
    "\n",
    "            return output_txt\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating PDF: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return f\"PDF translation error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "aa47982f7a2d43729158c2cb846d2b77",
      "7469f3fc345649d7aa4bd83cff08790e",
      "95a7417201c04adeab538d238581bb7e",
      "d84dace805004830a2f0a0ba155f8e8c",
      "d5508b88d7e84ca9b0101ca0f9e23ad2",
      "76bcc84e3ecb4e3e8c75607f8dd08048",
      "4e19e3ab1a714bb598bac3a6cdee6276",
      "7ed982f2a18e459b8637df68209a2a90",
      "c294a41ff11b4ef79c80a8b4a78f5664",
      "ae85daefeadb48e48d2c14a5074cb995",
      "45db328b2ffc4a7888d5bdeb6607f5a9",
      "cc9ec16b09a4433b9949c8684750003e",
      "9704fb4980004054a1588c5696e1bb66",
      "a62e07525d964d5b89f692c1e1c90169",
      "5f9e90d96cd142f2a68792a351ae5746",
      "12528d6a8be042ecb01e5082bed56564",
      "3e74743fbd0149fdbdf006f104b412c8",
      "f94dfda854274b66bda799709c40d80d",
      "21134b169643475b8767f5eaf8a41479",
      "0fbe335acdc94d549cc1089aa8aafbd2",
      "cbba973fb80c43298aa3aa6ccac50ab4",
      "795c4d7e24e84357969f4ca9d9e26160",
      "1f6cb79ee68c4530ba61cdbb492db330",
      "3fb959dfbdf34e0aa120a4a42e980c62",
      "afbc48629935452ab73b4800d1ff39a8",
      "e031270c473448d3acd703c4db91ac53",
      "64d470b3cab14d7d89918b19020882a2",
      "48117d6196fd4dc68bfc5ff34e8ed8dd",
      "3050cd92fc234cc19467c8049e54e1b8",
      "6c74394cd8e34fb681050277449dd7e3",
      "4041feaff156418baabd6f5ce0aa2f55",
      "423a2af65ea544208cfc5fa5cc623e4e",
      "46aea010b6d645faace1f935b7ba8876",
      "6c29aecf01024f77b14068a4a36c806c",
      "fa8c7ee4349e4aeda1da881d12ba187c",
      "39ac215717b2424caed9c09827b28170",
      "10c7291c3fb649bda4a8abf77fbeb39a",
      "ad6a7b7235ce4ba3bc82e65a16d3cc19",
      "57dc84debbec4d68a89165833564bd3b",
      "ad314e87b70a488784148aef2f4d8c86",
      "ded1dd921ccc42268f363a6a2ed1976c",
      "d84149e7b0cd4b798d7970e3e102daeb",
      "dd75cdce4ff143098ada7cde8d7e20f8",
      "76ebae3315b94618a01219fed6077ef2",
      "f15a235d4f16444399ad8c3354bb3005",
      "3327f155737f4566bef4fd87837f2544",
      "dde3310f6972448c9712589daa8e8bad",
      "fa00b1a4fdda4f63962216900d122479",
      "9d7c311d9ce745048d773d9ee9577b47",
      "bd181799e628432dacb8c3135ca2e503",
      "3a08afd652ed40c3a1fea566f950804a",
      "93b46b06618047dba17240cf6e5d767d",
      "83eeeb9a929847cbb71a03e5611df63c",
      "66bb5e731aa34211870a13303d0f2192",
      "106328586b49463f8ab8e1c6046de72c",
      "3579531daf8c4d989eaa26984068ec81",
      "df5b1fe5ce844051abf147d4dc8d7089",
      "f499fcb40a5b4783aa1d552d2a151089",
      "8a05c8a6d95845c8b5f16bc9ad36222e",
      "909bcb3bf38d4c5184f3a18f198e3e2d",
      "1fad12a7dfb743f39835d964144c8ec5",
      "2c7cc874fcdf41dc995668a1b408228b",
      "f327a191705840de9177486fdbf0a922",
      "f20a7e86620c44d4aa54e056fe680d30",
      "58cdc30e19a149638394f86df9227c72",
      "c5a577b68ce0461ab89b5d7c81ce22a8",
      "af17b673107d437796149fd2378ac08e",
      "b3dabc286a2d4099a71172cb41431aca",
      "b38718f90fdd4eb1850c5c574702abfe",
      "b015e8536b9e4ae4a9609e28a25dea92",
      "4f375c3b475c4609b182e2a42df01594",
      "bfd95242d18b4f7fb06e0713e711692c",
      "b971252dbafb4a88972b2924b4b305ff",
      "c1145a0cbb5645d4a97ee899ecf24c8a",
      "45bcdf18650b40478759a2718753095b",
      "1b76b9fc7b474b348e8cfb7fcd8fcbf2",
      "6f1d6466990e4d5ba5a563a6d8c07117",
      "9d4848533a884844b341b34d335d892e"
     ]
    },
    "id": "3SxymNC62gqR",
    "outputId": "b5a90215-6c09-49d4-9f75-5506632d8a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Initializing translation service with model facebook/nllb-200-distilled-600M on cuda-----\n",
      "\n",
      "-----Loading translation model: facebook/nllb-200-distilled-600M-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa47982f7a2d43729158c2cb846d2b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9ec16b09a4433b9949c8684750003e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6cb79ee68c4530ba61cdbb492db330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c29aecf01024f77b14068a4a36c806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15a235d4f16444399ad8c3354bb3005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579531daf8c4d989eaa26984068ec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af17b673107d437796149fd2378ac08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dabc286a2d4099a71172cb41431aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translator = TranslationService(\n",
    "    model_name = \"facebook/nllb-200-distilled-600M\",\n",
    "    device = \"auto\",\n",
    "    preserve_formatting = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNbKZ-2Enn2e"
   },
   "source": [
    "**English to French**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lhap_yh9bQJ6",
    "outputId": "0d427b3b-5394-4892-a2d6-e3bcc711a5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 2.19s (7.30 tokens/sec)\n",
      "\n",
      "   Total (in+out): 33 tokens at 15.05 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: Le renard brun rapide saute sur le chien paresseux.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating a simple sentence from English to French\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgtBgmuznsif"
   },
   "source": [
    "**English to Arabic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT_RgmyVc_U7",
    "outputId": "43d137cd-967d-4fa2-9e22-a745029f5a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 1.04s (15.42 tokens/sec)\n",
      "\n",
      "   Total (in+out): 35 tokens at 33.73 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: الثعلب البني السريع يقفز فوق الكلب الكسول.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating to Arabic\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"ar\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AORullonxBW"
   },
   "source": [
    "**English to Russian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GiFrJ9lg3ff",
    "outputId": "f9b11456-605c-4c5b-d589-58b0a8bd21fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 2.28s (7.03 tokens/sec)\n",
      "\n",
      "   Total (in+out): 35 tokens at 15.38 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: Быстрая коричневая лиса прыгает над ленивой собакой.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating to Russian\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"ru\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWkbfvcraaF5"
   },
   "source": [
    "**Instead of directly translating entire files, let's perform the translation on the chunks created during the chunking process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jwWYF2Y34qS",
    "outputId": "c1932820-be55-4619-b00c-57fdcfe59b86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/chunked_data.zip\n",
      "   creating: content/chunked_data/\n",
      "  inflating: content/chunked_data/The-Alchemist_chunks.json  \n",
      "  inflating: content/chunked_data/Ocean_ecogeochemistry_A_review_chunks.json  \n",
      "  inflating: content/chunked_data/Stats_chunks.json  \n",
      "  inflating: content/chunked_data/new-approaches-and-procedures-for-cancer-treatment_chunks.json  \n",
      "  inflating: content/chunked_data/all_chunks.json  \n",
      "  inflating: content/chunked_data/The_Plan_of_the_Giza_Pyramids_chunks.json  \n",
      "  inflating: content/chunked_data/M.Sc. Applied Psychology_chunks.json  \n",
      "  inflating: content/chunked_data/Dataset summaries and citations_chunks.json  \n"
     ]
    }
   ],
   "source": [
    "# Unzipping chunked data\n",
    "!unzip /content/chunked_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84OudBQ937Ig"
   },
   "outputs": [],
   "source": [
    "!mkdir chunked_data\n",
    "!mv /content/content/chunked_data/* /content/chunked_data\n",
    "!rm -rf /content/content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuRJT_PGdeCR"
   },
   "source": [
    "### Loading and Grouping Document Chunks by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kppazHy4Ug6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chunks_dir = \"/content/chunked_data\"\n",
    "def load_chunks(filename: str = \"all_chunks.json\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load document chunks from a JSON file.\"\"\"\n",
    "    filepath = os.path.join(chunks_dir, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks from {filepath}\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunks from {filepath}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFUJarnu4EqA"
   },
   "outputs": [],
   "source": [
    "def group_chunks_by_source(chunks: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Group chunks by their source document.\"\"\"\n",
    "    grouped = {}\n",
    "    for chunk in chunks:\n",
    "        source = chunk.get(\"source\", \"unknown\")\n",
    "        if source not in grouped:\n",
    "            grouped[source] = []\n",
    "        grouped[source].append(chunk)\n",
    "\n",
    "    # Sort chunks by chunk number within each source\n",
    "    for source in grouped:\n",
    "        grouped[source].sort(key=lambda x: x.get(\"chunk_number\", 0))\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1A708Hy4M5u",
    "outputId": "e8d5057b-03ac-4d6d-cd24-d9bbbbb55b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 chunks from /content/chunked_data/all_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# Group chunks by source\n",
    "chunks = load_chunks()\n",
    "grouped_chunks = group_chunks_by_source(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6po55AC4hp4",
    "outputId": "5c8dbaaf-5169-49fb-f3e8-80503bb69bc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['new-approaches-and-procedures-for-cancer-treatment.pdf', 'The_Plan_of_the_Giza_Pyramids.pdf', 'The-Alchemist.pdf', 'Stats.docx', 'Ocean_ecogeochemistry_A_review.pdf', 'Dataset summaries and citations.docx', 'M.Sc. Applied Psychology.docx'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_chunks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq4SoZ5xdvki"
   },
   "source": [
    "Now let's try translation from 'Stats.docx' document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "pJvaGTvg4kRQ",
    "outputId": "03762f51-9cfb-4412-f1e8-61cbc1d0cf30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Introduction The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs. The file provides a list of envelope and non-envelope components (e.g., Windows, Water Heaters) and any associated classes within those components (e.g., Low Emissivity, Electric Instantaneous).'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_chunks['Stats.docx'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVEvdYP04xJZ",
    "outputId": "f049d6f2-fc36-4994-d94d-87859e24a948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Downloading fastText language detection model...-----\n",
      "\n",
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 8.98s (14.93 tokens/sec)\n",
      "   Total (in+out): 282 tokens at 31.41 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to Arabic\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "ricumFXg-UAx",
    "outputId": "d6fe2783-7eb9-4999-9aa0-421b1969a085"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'مقدمة الغرض من ملف القراءة الآلية (the file) هو توفير معايير التراجع والقاطع للمكونات المختلفة لحساب تقديرات أسعار المواد المنخفضة والمتوسطة والعالية (10 و50 و90 مئوية) ومضاعفات العمالة / إضافات لتقدير تكاليف المشروعات الجديدة في البناء والتعديل. يقدم الملف قائمة بالمكونات الغلافية وغير الغلافية (مثل Windows و Heaters Water) وأي فئات مرتبطة ضمن تلك المكونات (مثل Low Emissivity و Electric Instantaneous).'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dzn9pr0-d4h",
    "outputId": "e5e865fd-f007-4c47-b82e-1fcba04bdaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 10.07s (13.31 tokens/sec)\n",
      "   Total (in+out): 236 tokens at 23.44 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to French\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "TXCiWT_I-YrA",
    "outputId": "5f3635e1-bd98-41c9-c3c2-55b82659382e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Introduction Le but du fichier Machine Readable (the file) est de fournir des coefficients de régression et des interceptions pour différents composants pour calculer les estimations basses, moyennes et élevées (10e, 50e et 90e percentiles) des prix des matériaux et des multiplicateurs de main-d'œuvre / add-ons pour estimer les coûts de nouveaux projets de construction et de rénovation.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqglZ75ABH46",
    "outputId": "b310d700-ac06-4a66-8b02-a43da8727576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 7.91s (16.95 tokens/sec)\n",
      "   Total (in+out): 267 tokens at 33.77 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to Japanese\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "IS_UXFOHBLGx",
    "outputId": "b72f0c67-46d9-4bf6-ffab-ddab3281e0ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'導入 マシンリーダブルファイル (the file) の目的は,低,中,高 (10 番目の,50 番目の,90 番目の百分点) の材料価格推定と労働倍数/アドオンを計算するために,異なるコンポーネントの回帰系数と傍受を提供することです.このファイルは,新しい建設と改装プロジェクトのコストを推定するために,包装および非包装コンポーネント (例えば,Windows,Water Heaters) とそれらのコンポーネント内の関連クラス (例えば,Low Emissivity, Electric Instantaneous) のリストを提供します.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw82HlOXePRM"
   },
   "source": [
    "\n",
    "\n",
    "> We can translate text into a wide range of languages using the current model. The supported languages are defined in a dictionary **`iso_to_name`** that maps language names to their corresponding NLLB language codes. This dictionary is easily extendable, allowing us to add support for even more languages as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnavI-ZCeyHy"
   },
   "source": [
    "**Now let's try to tranlsate few chunks of 'The Plan of the Giza Pyramids' document but in json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkH1Ukk9CaYr",
    "outputId": "16acdaa8-240b-493f-91ed-f82cfaf5aa2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 19 tokens processed in 1.05s (18.02 tokens/sec)\n",
      "\n",
      "   Total (in+out): 32 tokens at 30.36 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 12 tokens processed in 1.15s (10.46 tokens/sec)\n",
      "\n",
      "   Total (in+out): 22 tokens at 19.17 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.64s (6.21 tokens/sec)\n",
      "\n",
      "   Total (in+out): 8 tokens at 12.42 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 7 tokens processed in 0.83s (8.47 tokens/sec)\n",
      "\n",
      "   Total (in+out): 14 tokens at 16.94 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 19 tokens processed in 1.32s (14.44 tokens/sec)\n",
      "\n",
      "   Total (in+out): 32 tokens at 24.33 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 450 tokens processed in 64.89s (6.93 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1474 tokens at 22.71 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.65 tokens/sec)\n",
      "\n",
      "   Total (in+out): 11 tokens at 26.54 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 3 tokens processed in 0.27s (11.20 tokens/sec)\n",
      "\n",
      "   Total (in+out): 7 tokens at 26.13 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 6 tokens processed in 0.36s (16.63 tokens/sec)\n",
      "\n",
      "   Total (in+out): 13 tokens at 36.03 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 19 tokens processed in 0.64s (29.86 tokens/sec)\n",
      "\n",
      "   Total (in+out): 32 tokens at 50.28 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 231 tokens processed in 11.87s (19.46 tokens/sec)\n",
      "\n",
      "   Total (in+out): 402 tokens at 33.86 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.72 tokens/sec)\n",
      "\n",
      "   Total (in+out): 11 tokens at 26.73 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.63 tokens/sec)\n",
      "\n",
      "   Total (in+out): 7 tokens at 24.81 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 6 tokens processed in 0.37s (16.04 tokens/sec)\n",
      "\n",
      "   Total (in+out): 13 tokens at 34.76 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 19 tokens processed in 0.67s (28.37 tokens/sec)\n",
      "\n",
      "   Total (in+out): 32 tokens at 47.78 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 474 tokens processed in 15.47s (30.64 tokens/sec)\n",
      "\n",
      "   Total (in+out): 693 tokens at 44.79 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.56s (7.10 tokens/sec)\n",
      "\n",
      "   Total (in+out): 11 tokens at 19.51 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.29s (14.03 tokens/sec)\n",
      "\n",
      "   Total (in+out): 8 tokens at 28.06 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 7 tokens processed in 0.43s (16.43 tokens/sec)\n",
      "\n",
      "   Total (in+out): 14 tokens at 32.86 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 3 tokens processed in 0.29s (10.22 tokens/sec)\n",
      "\n",
      "   Total (in+out): 7 tokens at 23.84 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 6 tokens processed in 0.38s (15.92 tokens/sec)\n",
      "\n",
      "   Total (in+out): 13 tokens at 34.49 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 19 tokens processed in 0.67s (28.45 tokens/sec)\n",
      "\n",
      "   Total (in+out): 32 tokens at 47.92 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 174 tokens processed in 10.69s (16.28 tokens/sec)\n",
      "\n",
      "   Total (in+out): 375 tokens at 35.09 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 6 tokens processed in 0.38s (15.60 tokens/sec)\n",
      "\n",
      "   Total (in+out): 13 tokens at 33.81 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 4 tokens processed in 0.28s (14.15 tokens/sec)\n",
      "\n",
      "   Total (in+out): 8 tokens at 28.30 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 3 tokens processed in 0.29s (10.51 tokens/sec)\n",
      "\n",
      "   Total (in+out): 7 tokens at 24.52 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 7 tokens processed in 0.46s (15.28 tokens/sec)\n",
      "\n",
      "   Total (in+out): 14 tokens at 30.57 tokens/sec\n",
      "\n",
      "\n",
      "⏱️ Translation: 212 tokens processed in 13.43s (15.78 tokens/sec)\n",
      "\n",
      "   Total (in+out): 451 tokens at 33.57 tokens/sec\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translated_jsons = []\n",
    "for chunk in grouped_chunks['The_Plan_of_the_Giza_Pyramids.pdf'][:5]: # First 5 chunks\n",
    "    translated_json_data = translator._translate_json(chunk, target_language=\"ar\", source_language=\"en\")\n",
    "    translated_jsons.append(translated_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ixigQcPB1fq",
    "outputId": "4e7c727d-2e30-4f76-e90d-ba76b0891988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Chunk:\n",
      "{'source': 'The_Plan_of_the_Giza_Pyramids.pdf', 'pages': [1], 'chunk_number': 0, 'text': 'The Plan of the Giza Pyramids 1', 'token_count': 9, 'element_types': ['header', 'uncategorizedtext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'خطة الأهرامات في غيزا.pdf', 'pages': [1], 'chunk_number': 0, 'text': 'خطة أهرامات غزة 1', 'token_count': 9, 'element_types': ['الرأس', 'النص غير المصنف']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'The_Plan_of_the_Giza_Pyramids.pdf', 'pages': [1], 'chunk_number': 1, 'text': \"The Plan of the Giza Pyramids by John A.R. Legon Revision Date: 22-11-2019 My initial findings on the Giza Site Plan of Three Pyramids were first briefly summarized in a pamphlet published by the Archaeology Society of Staten Island in 1979.1 Subsequently, I described the most significant features of the integrated site plan in several articles in the journals Discussions in Egyptology2 and Göttinger Miszellen.3 Further research has shown that many more factors must be taken into account, without alterating the basic framework of the dimensions in royal cubits which I described in 1979. The following text is based on my article in Discussions in Egyptology Vol. 10, but now includes much new material and new illustrations. In addition, acount has been taken of the extensive survey work carried out since 2012 by the late Glen Dash, and the positions of the corners of the three pyramids have been placed in the coordinate system initiated by Mark Lehner and David Goodman for the Giza Plateau Mapping Project. Now that a detailed topographical study of the Giza Plateau is in progress, 4 it is interesting to consider the results of the excavations and survey carried out by Flinders Petrie in 1880-2, when the exact dimensions and relative positions of the pyramids of Khufu, Khaefre and Menkaure, were established by triangulation..5 With reference to Petrie's survey-data, the present paper reviews the evidence first put forward by the writer in 1979, 6 showing that the sizes and relative positions of the three pyramids were determined by a single unifying ground plan. 1 J.A.R. Legon, 'The Plan of the Giza Pyramids', Archaeological Reports of the Archaeology Society of Staten Island, Vol.10 No.1. New York, 1979. 2 J.A.R. Legon, 'A Ground Plan at Giza', DE 10 (1988), 33-40; 'The Giza Ground Plan and Sphinx', DE 14 (1989), 53- 60.\", 'token_count': 441, 'element_types': ['footer', 'title', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'خطة الأهرامات في غيزا.pdf', 'pages': [1], 'chunk_number': 1, 'text': 'خطة الهرمات الجيزية من جون أ. آر. ليغون تاريخ مراجعة: 22-11-2019 تم تلخيص نتائجي الأولية حول خطة موقع الجيزة للثلاث الهرمات لأول مرة بشكل قصير في كتيب نشرته جمعية الآثار في جزيرة ستاتن في عام 1979.1 لاحقاً، وصفت أهم ميزات خطة الموقع المتكاملة في العديد من المقالات في مجلات مناقشات في مصرولوجيا2 وGöttinger Miszellen.3 أظهرت أبحاث أخرى أن هناك العديد من العوامل الأخرى التي يجب أخذها في الاعتبار ، دون تغيير الإطار الأساسي للأبعاد في الكوبات التي وصفتها في عام 1979. يعتمد النص على مقالتي في مناقشات في طبعة 10 من علم الآثار في جزيرة ستاتن ، ولكن الآن يتضمن الكثير من المواد والتوضيحات الجديدة. بالإضافة إلى ذلك ، تم تحديد نتائج عمل واسع النطاق الذي قام به مجتمع الجيزة الجيزية منذ أواخر عام 1979 ، وتم تحديد نتائج الدراسة التفصيلية التي أجريت من قبل ديفيد ديفيد غراودس و ديشينغرايدز (1989). تم تحديد مواقف ثلاثة ركنات من أساسية في خطة الجيزة الجيزة الجيزية ، والتي تم تحديدها من قبل خطة الجيزة الجيزة الجيزية والجيزة الجيزية ، والجيزة الجيزة الجيزة الجيزية، والجيزة الجيزية، والجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزية، والجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة، والجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة الجيزة، والجيزة، والجيزةجيزةجيزة، والجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزة، والجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزة، والجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيةجيزةجيةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيزةجيةجية،', 'token_count': 441, 'element_types': ['أقدام القدم', 'العنوان', 'النص الترويجي']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'The_Plan_of_the_Giza_Pyramids.pdf', 'pages': [1], 'chunk_number': 2, 'text': \"2 J.A.R. Legon, 'A Ground Plan at Giza', DE 10 (1988), 33-40; 'The Giza Ground Plan and Sphinx', DE 14 (1989), 53- 60. 3 . J.A.R. Legon, 'The Design of the Pyramid of Khaefre', GM 110 (1989), 27-34; 'The Geometry of the Bent Pyramid', GM 116 (1990) 65-72, 71; 'The Giza Site Plan Revisited', GM 124 (1991), 69-78. 4 1. M. Lehner, 'The Development of the Giza Necropolis: The Khufu Project', MDAIK 41, 1985, 109-143. 5 2. W.M.F. Petrie, The Pyramids and Temples of Gizeh (London, 1883). First edition only for full details, 34-36. 6 J.A.R. Legon, 'The Plan of the Giza Pyramids', Archaeological Reports of the Archaeology Society of Staten Island, Vol.10 No.1. New York, 1979.\", 'token_count': 254, 'element_types': ['footer', 'title', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'خطة الأهرامات في غيزا.pdf', 'pages': [1], 'chunk_number': 2, 'text': \"2 J.A.R. Legon, 'خطة أرض في غزة', DE 10 (1988), 33-40; 'خطة أرض غيزا وسفينكس', DE 14 (1989), 53-60. 3. J.A.R. Legon, 'تصميم هرم خفر', GM 110 (1989), 27-34; 'جيماتية الهرم المنحوت', GM 116 (1990) 65-72, 71; 'خطة موقع غيزا تم مراجعتها', GM 124 (1991), 69-78. 4 1. M. Lehner, 'تنمية حرم غيزا: مشروع خوفو', MDAIK 41, 1985, 109-143. 5 2. W.M.F. Petrie, The Pyramids and Temples of Gizeh (لندن, 1883).\", 'token_count': 254, 'element_types': ['أقدام القدم', 'العنوان', 'النص الترويجي']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'The_Plan_of_the_Giza_Pyramids.pdf', 'pages': [2], 'chunk_number': 3, 'text': 'The Plan of the Giza Pyramids 2 PLAN OF THE TRIANGULATION OF THE GIZA, PYRAMIDS CARRIED OUT BY Wim. FLINDERS PETRIE IN 1881-2 The existence of a dimensional scheme underlying the placing of the three pyramids is suggested in the first instance by the very regular arrangement of these pyramids on the Giza plateau. As a result, the sides of the bases and the distances that separate them define consecutive axial distances from north to south and from east to west. The three pyramids were accurately aligned with respect to the four cardinal points, and were displaced from one another in a configuration which satisfies the requirements of a coherent dimensional design. Certain technical difficulties relating to the site chosen for each pyramid in turn also suggest that there must have been some significant constraint, in addition to factors such as ease of construction or the selection of the most favourable architectural setting, which determined where each of the three pyramids was positioned. Using some of the finest surveying equipment available in his day, Petrie asserted that he had fixed the positions of the main stations in his triangulation to within 3 mm.7 The accuracy of his work is proven by the fact that his result for the mean side-length of the Great Pyramid differs from the value obtained in the meticulous survey carried out by J.H. Cole in 1925,8 by only 1.5 cm, even though nearly all of the outer casing of the pyramid is missing. Whereas Petrie had located single points of the casing in pits near the centre of each side, excavations in preparation for the work by Cole showed that substantial traces of the original casing-edge still remained on the pavement in some places where the casing itself has been destroyed. 7 Petrie, op.cit., 24. 8 .J-H..Cole, The determination of the exact size and orientation of the Great Pyramid of Giza, (Survey of Egypt, paper no.39), (Cairo, 1925).', 'token_count': 405, 'element_types': ['footer', 'header', 'uncategorizedtext', 'image', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'خطة الأهرامات في غيزا.pdf', 'pages': [2], 'chunk_number': 3, 'text': 'مخطط الهرمات الجيزية 2 خطة تثليث غيزة، تحدد الهرمات المسافات المحورية المتتالية من الشمال إلى الجنوب ومن الشرق إلى الغرب. تم نقل الهرمات الثلاثة بدقة فيما يتعلق بالنقاط الأساسية الأربعة، وتم تحويلها عن بعضها البعض في تكوين يلبي متطلبات التصميم المتماشى. لا تزال هناك بعض الصعوبات المتعلقة بالهرمات المختارة في مصر التي حصل عليها في بدوره، وبالإضافة إلى بعض المواقع التي تم تحديدها من جانبها، حيث تم تحديد بعض المواقع المختلفة، على الرغم من أنه لا يزال هناك بعض القيمات المحددة التي تم تحديدها من جانبها. في عام 1925, تم تحديد بعض المواقع التي تم تحديدها من جانبها، على الرغم من وجود بعض القيمات المختلفة التي تم تحديدها من جانبها.', 'token_count': 405, 'element_types': ['أقدام القدم', 'الرأس', 'النص غير المصنف', 'الصورة', 'النص الترويجي']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'The_Plan_of_the_Giza_Pyramids.pdf', 'pages': [3], 'chunk_number': 4, 'text': 'The Plan of the Giza Pyramids 3 The dimensions of the three pyramid-bases as determined by Petrie are given in Table I, together with the average variations in the lengths of the sides and the orientations of the three pyramids with respect to true north. The distances separating the centres of the pyramids were computed by Petrie along axes constructed parallel to the mean azimuth of the Second and Great Pyramids of -4\\' 52\",9 and are given in Table II.\\n\\nPetrie: Inches Royal Cubits Azimuth Great Pyramid 9068.8 ± 0.6 439.81 -3\\' 43” Second Pyramid 8474.9 ± 1.5 411.00 -5\\' 26” Third Pyramid 4153.6 ± 3.0 201.44 14\\' 03”', 'token_count': 172, 'element_types': ['narrativetext', 'header', 'table', 'uncategorizedtext'], 'tables': [\"<table><thead><tr><th>Table I</th><th>Petrie: Inches</th><th>| Royal Cubits</th><th>Azimuth</th></tr></thead><tbody><tr><td>Great Pyramid</td><td>9068.8 + 0.6</td><td>439.81</td><td>-3' 43”</td></tr><tr><td>Second Pyramid</td><td>8474.9 + 1.5</td><td>411.00</td><td>-5' 26”</td></tr><tr><td>Third Pyramid</td><td>4153.6 + 3.0</td><td>201.44</td><td>14' 03”</td></tr></tbody></table>\"]}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'خطة الأهرامات في غيزا.pdf', 'pages': [3], 'chunk_number': 4, 'text': 'خطة الهرمات الجيزية 3 أبعاد ثلاثة أسس الهرمات التي حددها بيتري هي المقدمة في الجدول الأول ، جنبا إلى جنب مع المتغيرات المتوسطة في أطول الجانبين وتوجهات الهرمات الثلاثة فيما يتعلق بالشمال الحقيقي.\\n\\nتم حساب المسافات التي تفصل بين مراكز الهرمات من قبل بيتري على طول محور بنيت بالتوازي مع متوسط أزموت الهرمات الثانية والعظيمة من -4\\' 52\",9 ويتم تقديمها في الجدول الثاني.\\n\\nبيتري: بوصة الكوبيت الملكية أزموت الهرمة العظيمة 9068.8 ± 0.6 439.81 -3\\' 43 الهرمة الثانية 8474.9 ± 1.5 411.00 -5\\' 26 الهرمة الثالثة 4153.6 3.0.44 03\\' 03.', 'token_count': 172, 'element_types': ['النص الترويجي', 'الرأس', 'الطاولة', 'النص غير المصنف'], 'tables': ['الجدول الثاني الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث الثالث']}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for original, translated in zip(grouped_chunks['The_Plan_of_the_Giza_Pyramids.pdf'], translated_jsons):\n",
    "    print(\"Original Chunk:\")\n",
    "    print(original)\n",
    "    print(\"\\nTranslated Chunk:\")\n",
    "    print(translated)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l158Ymzrgx5i"
   },
   "source": [
    "#### **Translating entire file now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BXOQ4UeEHtHZ",
    "outputId": "e38dc9eb-9d57-46b1-8a0b-41a03d858234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 5 tokens processed in 0.67s (7.50 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 13.49 tokens/sec\n",
      "⏱️ Translation: 132 tokens processed in 10.60s (12.46 tokens/sec)\n",
      "   Total (in+out): 278 tokens at 26.24 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.57s (12.20 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 29.64 tokens/sec\n",
      "⏱️ Translation: 62 tokens processed in 3.38s (18.35 tokens/sec)\n",
      "   Total (in+out): 119 tokens at 35.21 tokens/sec\n",
      "⏱️ Translation: 249 tokens processed in 16.90s (14.73 tokens/sec)\n",
      "   Total (in+out): 520 tokens at 30.77 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.97s (8.23 tokens/sec)\n",
      "   Total (in+out): 19 tokens at 19.55 tokens/sec\n",
      "⏱️ Translation: 161 tokens processed in 12.06s (13.35 tokens/sec)\n",
      "   Total (in+out): 362 tokens at 30.02 tokens/sec\n",
      "⏱️ Translation: 86 tokens processed in 6.56s (13.11 tokens/sec)\n",
      "   Total (in+out): 206 tokens at 31.40 tokens/sec\n",
      "⏱️ Translation: 107 tokens processed in 10.41s (10.28 tokens/sec)\n",
      "   Total (in+out): 288 tokens at 27.67 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.56s (8.98 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 25.14 tokens/sec\n",
      "⏱️ Translation: 75 tokens processed in 6.06s (12.37 tokens/sec)\n",
      "   Total (in+out): 178 tokens at 29.37 tokens/sec\n",
      "⏱️ Translation: 26 tokens processed in 2.44s (10.66 tokens/sec)\n",
      "   Total (in+out): 68 tokens at 27.88 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.66s (12.04 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 25.58 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.75 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 24.38 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.46s (6.47 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 17.25 tokens/sec\n",
      "⏱️ Translation: 13 tokens processed in 0.79s (16.51 tokens/sec)\n",
      "   Total (in+out): 25 tokens at 31.74 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.69s (10.13 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 24.59 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.59s (11.85 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 27.10 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.48s (16.54 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 31.01 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.83s (13.18 tokens/sec)\n",
      "   Total (in+out): 26 tokens at 31.15 tokens/sec\n",
      "⏱️ Translation: 10 tokens processed in 0.67s (14.99 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 32.97 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.75s (11.99 tokens/sec)\n",
      "   Total (in+out): 21 tokens at 27.97 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.42s (21.32 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.54 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.81s (9.91 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 22.29 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.60s (11.61 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 24.88 tokens/sec\n",
      "⏱️ Translation: 260 tokens processed in 19.39s (13.41 tokens/sec)\n",
      "   Total (in+out): 461 tokens at 23.78 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.46s (15.33 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 30.65 tokens/sec\n",
      "⏱️ Translation: 22 tokens processed in 1.46s (15.07 tokens/sec)\n",
      "   Total (in+out): 48 tokens at 32.89 tokens/sec\n",
      "⏱️ Translation: 74 tokens processed in 6.85s (10.80 tokens/sec)\n",
      "   Total (in+out): 186 tokens at 27.15 tokens/sec\n",
      "⏱️ Translation: 30 tokens processed in 2.02s (14.88 tokens/sec)\n",
      "   Total (in+out): 67 tokens at 33.23 tokens/sec\n",
      "⏱️ Translation: 35 tokens processed in 2.80s (12.49 tokens/sec)\n",
      "   Total (in+out): 85 tokens at 30.33 tokens/sec\n",
      "⏱️ Translation: 108 tokens processed in 9.06s (11.92 tokens/sec)\n",
      "   Total (in+out): 263 tokens at 29.02 tokens/sec\n",
      "⏱️ Translation: 25 tokens processed in 1.81s (13.83 tokens/sec)\n",
      "   Total (in+out): 47 tokens at 26.00 tokens/sec\n",
      "⏱️ Translation: 92 tokens processed in 7.00s (13.14 tokens/sec)\n",
      "   Total (in+out): 208 tokens at 29.70 tokens/sec\n",
      "⏱️ Translation: 25 tokens processed in 1.41s (17.77 tokens/sec)\n",
      "   Total (in+out): 50 tokens at 35.54 tokens/sec\n",
      "⏱️ Translation: 36 tokens processed in 2.74s (13.15 tokens/sec)\n",
      "   Total (in+out): 86 tokens at 31.41 tokens/sec\n",
      "⏱️ Translation: 42 tokens processed in 3.07s (13.69 tokens/sec)\n",
      "   Total (in+out): 96 tokens at 31.30 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.65s (9.21 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 21.48 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.58s (13.80 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 27.61 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.68s (11.71 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 23.43 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.29s (13.75 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 27.50 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.32s (9.33 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 21.77 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.36 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 27.20 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.49s (10.22 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 26.57 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.79s (11.43 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 27.95 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.27s (14.66 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 29.32 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.07s (13.14 tokens/sec)\n",
      "   Total (in+out): 31 tokens at 29.09 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.94s (18.00 tokens/sec)\n",
      "   Total (in+out): 34 tokens at 35.99 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.91s (18.65 tokens/sec)\n",
      "   Total (in+out): 33 tokens at 36.21 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.87s (18.39 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.78 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (18.00 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 35.99 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (17.93 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 35.86 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.04s (15.44 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 30.88 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.67s (11.93 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 25.34 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.66s (12.12 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 22.73 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.42s (18.96 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.55 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.34s (11.85 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 29.62 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.36s (8.35 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 19.48 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.65 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 30.35 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.26 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 24.53 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.35s (14.41 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 28.82 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.36s (13.79 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.59 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.36s (13.76 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.52 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.67s (5.93 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 14.82 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.32s (12.58 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 28.31 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.83 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.65 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.33s (11.94 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 23.88 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.07s (13.05 tokens/sec)\n",
      "   Total (in+out): 31 tokens at 28.89 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.95s (17.99 tokens/sec)\n",
      "   Total (in+out): 34 tokens at 35.97 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.95s (17.96 tokens/sec)\n",
      "   Total (in+out): 33 tokens at 34.87 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.88s (18.11 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.23 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.13s (14.21 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 28.41 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.11s (14.46 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 28.93 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (18.01 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.02 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.53s (15.11 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 32.11 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.46s (17.24 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 32.32 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.42s (18.96 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.56 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (11.04 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.61 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.38s (7.99 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 18.63 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.42s (12.01 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 28.83 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.48s (12.51 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 25.02 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.42s (9.47 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 18.94 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.30s (13.43 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 26.86 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.32s (12.53 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 25.06 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.47s (10.66 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 23.46 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (11.06 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.65 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.32s (9.33 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 18.66 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.82 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.65 tokens/sec\n",
      "⏱️ Translation: 12 tokens processed in 0.90s (13.33 tokens/sec)\n",
      "   Total (in+out): 23 tokens at 25.56 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.65s (16.94 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 33.88 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.83s (13.22 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 26.45 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.58s (10.37 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 20.75 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.16 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 22.29 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.46s (13.04 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 30.42 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.37s (13.43 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 26.85 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.39s (12.73 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 25.46 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.42s (14.24 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 28.48 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.52s (15.40 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 32.73 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.59s (15.32 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 30.65 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.42s (11.81 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 23.61 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.54s (9.34 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 22.41 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.76 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 19.52 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.70 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 24.25 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.52s (13.55 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 30.96 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.51s (17.62 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 35.24 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.39s (12.70 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 25.40 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.52s (9.65 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 23.17 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.31s (13.06 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 26.11 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.61 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.21 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.53s (9.42 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 26.38 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.59s (15.34 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 30.69 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.74s (12.09 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 24.18 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.75s (11.97 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 23.94 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.70s (12.89 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 25.77 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.29s (13.71 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 30.85 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.58s (11.98 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 25.67 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.47 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 29.93 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.17 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 28.39 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.30s (13.17 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 29.63 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (10.97 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 21.94 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.67s (13.39 tokens/sec)\n",
      "   Total (in+out): 19 tokens at 28.27 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.47s (10.55 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 25.32 tokens/sec\n",
      "⏱️ Translation: 12 tokens processed in 0.99s (12.10 tokens/sec)\n",
      "   Total (in+out): 30 tokens at 30.26 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.05s (13.31 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 30.43 tokens/sec\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/translated/Stats_ar.docx'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate_file(file_path='/content/Stats.docx', target_language='ar', source_language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45WMA-kgifZr"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/UzairNaeem3/DrX_EnigmaticResearch/raw/master/images/Screenshot_8.png\" style=\"display: inline-block\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gmFHB8kjVJ-"
   },
   "source": [
    "**Not perfect, but better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIuoyG8Ui_Ti"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/UzairNaeem3/DrX_EnigmaticResearch/raw/master/images/Screenshot_9.png\" style=\"display: inline-block\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDhYs9bch_Tc"
   },
   "source": [
    "### **Translating a PDF file now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lLxRBhTkLDXr",
    "outputId": "183c8744-ea55-4999-91a4-58d646b0d5fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting page-by-page translation of PDF: /content/The_Plan_of_the_Giza_Pyramids.pdf\n",
      "PDF has 16 pages\n",
      "Processing page 1 of 16\n",
      "⏱️ Translation: 643 tokens processed in 26.64s (24.14 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1091 tokens at 40.96 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 1 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_1.txt\n",
      "Processing page 2 of 16\n",
      "⏱️ Translation: 438 tokens processed in 18.10s (24.20 tokens/sec)\n",
      "\n",
      "   Total (in+out): 737 tokens at 40.72 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 2 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_2.txt\n",
      "Processing page 3 of 16\n",
      "⏱️ Translation: 697 tokens processed in 37.14s (18.77 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1267 tokens at 34.11 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 3 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_3.txt\n",
      "Processing page 4 of 16\n",
      "⏱️ Translation: 803 tokens processed in 33.95s (23.65 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1240 tokens at 36.52 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 4 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_4.txt\n",
      "Processing page 5 of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 980 tokens processed in 111.31s (8.80 tokens/sec)\n",
      "\n",
      "   Total (in+out): 2182 tokens at 19.60 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 5 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_5.txt\n",
      "Processing page 6 of 16\n",
      "⏱️ Translation: 819 tokens processed in 74.48s (11.00 tokens/sec)\n",
      "\n",
      "   Total (in+out): 2137 tokens at 28.69 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 6 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_6.txt\n",
      "Processing page 7 of 16\n",
      "⏱️ Translation: 463 tokens processed in 19.44s (23.82 tokens/sec)\n",
      "\n",
      "   Total (in+out): 790 tokens at 40.64 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 7 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_7.txt\n",
      "Processing page 8 of 16\n",
      "⏱️ Translation: 333 tokens processed in 55.91s (5.96 tokens/sec)\n",
      "\n",
      "   Total (in+out): 614 tokens at 10.98 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 8 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_8.txt\n",
      "Processing page 9 of 16\n",
      "⏱️ Translation: 477 tokens processed in 56.09s (8.50 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1454 tokens at 25.92 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 9 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_9.txt\n",
      "Processing page 10 of 16\n",
      "⏱️ Translation: 337 tokens processed in 42.30s (7.97 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1109 tokens at 26.22 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 10 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_10.txt\n",
      "Processing page 11 of 16\n",
      "⏱️ Translation: 689 tokens processed in 62.45s (11.03 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1768 tokens at 28.31 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 11 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_11.txt\n",
      "Processing page 12 of 16\n",
      "⏱️ Translation: 359 tokens processed in 55.73s (6.44 tokens/sec)\n",
      "\n",
      "   Total (in+out): 611 tokens at 10.96 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 12 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_12.txt\n",
      "Processing page 13 of 16\n",
      "⏱️ Translation: 274 tokens processed in 15.66s (17.50 tokens/sec)\n",
      "\n",
      "   Total (in+out): 511 tokens at 32.63 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 13 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_13.txt\n",
      "Processing page 14 of 16\n",
      "⏱️ Translation: 408 tokens processed in 16.92s (24.11 tokens/sec)\n",
      "\n",
      "   Total (in+out): 680 tokens at 40.19 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 14 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_14.txt\n",
      "Processing page 15 of 16\n",
      "⏱️ Translation: 765 tokens processed in 37.53s (20.38 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1330 tokens at 35.44 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 15 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_15.txt\n",
      "Processing page 16 of 16\n",
      "⏱️ Translation: 948 tokens processed in 44.64s (21.24 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1608 tokens at 36.02 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 16 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_16.txt\n",
      "Saved complete translated text to /content/translated/The_Plan_of_the_Giza_Pyramids_ar.txt\n",
      "Individual translated pages saved to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/translated/The_Plan_of_the_Giza_Pyramids_ar.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate_file(file_path='/content/The_Plan_of_the_Giza_Pyramids.pdf', target_language='ar', source_language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4gN4AEpsBRb"
   },
   "source": [
    "\n",
    "\n",
    "> It does translate the PDF, but after translation, it wasn't converting back into a PDF correctly. So, I saved the translated text as a .txt file instead of a PDF. However, when dealing with complex PDFs that contain tables, the results were still not satisfactory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ow_ShLGJpv1u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
