{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYtvklBPvNiM",
    "outputId": "ce45669e-8f91-49b3-ea4b-b9209ff7bac2"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m745.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install torch fasttext python-docx PyPDF2 beautifulsoup4 reportlab bitsandbytes -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kkmyUXDABMQ",
    "outputId": "0c1d2f43-0a50-4bd8-9fa1-7d6567d350fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.0.2\n",
      "Uninstalling numpy-2.0.2:\n",
      "  Successfully uninstalled numpy-2.0.2\n",
      "Found existing installation: transformers 4.51.3\n",
      "Uninstalling transformers-4.51.3:\n",
      "  Successfully uninstalled transformers-4.51.3\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
      "peft 0.14.0 requires transformers, which is not installed.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m217.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "Successfully installed transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy transformers\n",
    "!pip install numpy==1.26.4\n",
    "!pip install transformers --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q33sJC_MbyXR"
   },
   "source": [
    "After installing the necessary libraries, it's important to restart the session. This ensures that the newly installed packages are properly loaded into the environment. Without restarting, the current session may not recognize the changes, and the libraries might not work as expected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N1uHmvGswjd3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUERLkqxgGiu"
   },
   "source": [
    "### **Decorator to measure tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6RB6nwo-Icax"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "from typing import Callable, Any, Optional\n",
    "\n",
    "def measure_token_processing(process_name: Optional[str] = None):\n",
    "    \"\"\"Decorator to measure token processing speed across different tasks.\"\"\"\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Determine the process name\n",
    "            task_name = process_name or func.__name__\n",
    "\n",
    "            # Get the instance (self) from args\n",
    "            instance = args[0] if args else None\n",
    "\n",
    "            # Find text to tokenize - could be in different places depending on function\n",
    "            text = None\n",
    "            # Check args - likely the second argument for most methods\n",
    "            if len(args) > 1 and isinstance(args[1], str):\n",
    "                text = args[1]\n",
    "            elif len(args) > 1 and isinstance(args[1], list) and all(isinstance(t, str) for t in args[1]):\n",
    "                text = ' '.join(args[1])  # Join list of strings\n",
    "\n",
    "            # Check kwargs for text\n",
    "            elif 'text' in kwargs and isinstance(kwargs['text'], str):\n",
    "                text = kwargs['text']\n",
    "            elif 'texts' in kwargs and isinstance(kwargs['texts'], list):\n",
    "                text = ' '.join(kwargs['texts'])\n",
    "            elif 'prompt' in kwargs and isinstance(kwargs['prompt'], str):\n",
    "                text = kwargs['prompt']\n",
    "\n",
    "            # Find appropriate tokenizer\n",
    "            tokenizer = None\n",
    "            if instance and hasattr(instance, 'tokenizer'):\n",
    "                tokenizer = instance.tokenizer\n",
    "            elif instance and hasattr(instance, 'embedding_model') and hasattr(instance.embedding_model, 'tokenizer'):\n",
    "                tokenizer = instance.embedding_model.tokenizer\n",
    "\n",
    "            # Count tokens if we have both text and tokenizer\n",
    "            input_tokens = 0\n",
    "            if text and tokenizer:\n",
    "                try:\n",
    "                    if hasattr(tokenizer, 'encode'):\n",
    "                        input_tokens = len(tokenizer.encode(text))\n",
    "                    elif hasattr(tokenizer, '__call__'):\n",
    "                        input_tokens = len(tokenizer(text)['input_ids'])\n",
    "                except:\n",
    "                    # Fallback to rough estimate (approx 4 chars per token)\n",
    "                    input_tokens = len(text) // 4\n",
    "            elif text:\n",
    "                # Very rough approximation if no tokenizer available\n",
    "                input_tokens = len(text.split())\n",
    "\n",
    "            # Start timing\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Execute the function\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            # End timing\n",
    "            end_time = time.perf_counter()\n",
    "\n",
    "            # Calculate tokens per second\n",
    "            processing_time = end_time - start_time\n",
    "            tokens_per_second = input_tokens / processing_time if processing_time > 0 else 0\n",
    "\n",
    "            # Output measurements\n",
    "            print(f\"⏱️ {task_name}: {input_tokens} tokens processed in {processing_time:.2f}s ({tokens_per_second:.2f} tokens/sec)\\n\")\n",
    "\n",
    "            # If result is a string, we could measure output tokens too\n",
    "            if isinstance(result, str) and tokenizer:\n",
    "                try:\n",
    "                    output_tokens = len(tokenizer.encode(result))\n",
    "                    total_tokens = input_tokens + output_tokens\n",
    "                    throughput = total_tokens / processing_time if processing_time > 0 else 0\n",
    "                    print(f\"   Total (in+out): {total_tokens} tokens at {throughput:.2f} tokens/sec\\n\\n\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFeKtBpPQgkC"
   },
   "source": [
    "### **Translation Service**\n",
    "\n",
    "A simple yet effective document translation system that uses local LLMs:\n",
    "\n",
    "- **Local Language Model Support**: Uses lightweight models like NLLB and M2M100 for translation\n",
    "- **Format Preservation**: Maintains document structure, paragraphs, lists, and tables during translation\n",
    "- **Chunking Strategy**: Intelligently splits text into manageable pieces while preserving semantic integrity\n",
    "- **Multi-Document Support**: Handles various formats including PDF, DOCX, and plain text\n",
    "- **Language Auto-Detection**: Automatically identifies source language when not specified\n",
    "- **Resource Optimization**: Implements model caching and fallback to smaller models when needed\n",
    "- **Table and Code Handling**: Preserves special content formats that shouldn't be fully translated\n",
    "\n",
    "This service provides efficient translation capabilities that can run entirely on local hardware without requiring external API calls or cloud services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stq9amsdgi2X"
   },
   "outputs": [],
   "source": [
    "# Login to HF to access LLMs\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ft3DHv_Q0SsA"
   },
   "outputs": [],
   "source": [
    "class TranslationService:\n",
    "    \"\"\"\n",
    "    A service for translating documents between languages using local LLMs.\n",
    "    Focuses on maintaining document structure and formatting during translation.\n",
    "    \"\"\"\n",
    "    # Class-level cache for loaded models\n",
    "    _loaded_translation_model = None\n",
    "    _loaded_translation_model_name = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"facebook/nllb-200-distilled-600M\",  # Smaller NLLB model suitable for local use\n",
    "        device: str = \"auto\",\n",
    "        preserve_formatting: bool = True\n",
    "    ):\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.preserve_formatting = preserve_formatting\n",
    "\n",
    "        # Determine device\n",
    "        if device == \"auto\":\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        print(f\"-----Initializing translation service with model {model_name} on {self.device}-----\\n\")\n",
    "\n",
    "        # Load translation model (with caching)\n",
    "        self._load_model()\n",
    "\n",
    "        # Map of language codes for NLLB\n",
    "        self.language_code_map = self._initialize_language_codes()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the translation model and tokenizer, with caching.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Check if the model is already loaded\n",
    "            if (TranslationService._loaded_translation_model is not None and\n",
    "                TranslationService._loaded_translation_model_name == self.model_name):\n",
    "                print(\"Translation model already loaded. Reusing from cache.\")\n",
    "                self.model = TranslationService._loaded_translation_model\n",
    "                self.tokenizer = self.model.tokenizer\n",
    "                return\n",
    "\n",
    "            # Import necessary libraries\n",
    "            from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "            print(f\"-----Loading translation model: {self.model_name}-----\\n\")\n",
    "\n",
    "            # Load the tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "\n",
    "            # Load model with appropriate settings based on device\n",
    "            if self.device == \"cuda\":\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,\n",
    "                    load_in_8bit=True\n",
    "                )\n",
    "            else:\n",
    "                self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map={\"\": self.device}\n",
    "                )\n",
    "\n",
    "            # Cache the loaded model\n",
    "            TranslationService._loaded_translation_model = self.model\n",
    "            TranslationService._loaded_translation_model_name = self.model_name\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading translation model: {str(e)}\")\n",
    "            print(\"-----Attempting to load a smaller fallback model...-----\\n\")\n",
    "\n",
    "            try:\n",
    "                # Fallback to an even smaller model\n",
    "                fallback_model = \"facebook/m2m100_418M\"\n",
    "                self.model_name = fallback_model\n",
    "\n",
    "                from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "                self.tokenizer = M2M100Tokenizer.from_pretrained(fallback_model)\n",
    "                self.model = M2M100ForConditionalGeneration.from_pretrained(\n",
    "                    fallback_model,\n",
    "                    device_map={\"\": self.device}\n",
    "                )\n",
    "\n",
    "                # Cache the fallback model\n",
    "                TranslationService._loaded_translation_model = self.model\n",
    "                TranslationService._loaded_translation_model_name = fallback_model\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"Error loading fallback model: {str(e2)}\")\n",
    "                print(\"-----Translation service will not be functional.-----\\n\")\n",
    "                self.model = None\n",
    "                self.tokenizer = None\n",
    "\n",
    "    def _initialize_language_codes(self):\n",
    "        \"\"\"Initialize the mapping of language names to NLLB language codes.\"\"\"\n",
    "\n",
    "        # This is a partial list of NLLB language codes - expand as needed\n",
    "        return {\n",
    "            \"english\": \"eng_Latn\",\n",
    "            \"arabic\": \"arb_Arab\",\n",
    "            \"french\": \"fra_Latn\",\n",
    "            \"spanish\": \"spa_Latn\",\n",
    "            \"german\": \"deu_Latn\",\n",
    "            \"chinese\": \"zho_Hans\",\n",
    "            \"japanese\": \"jpn_Jpan\",\n",
    "            \"russian\": \"rus_Cyrl\",\n",
    "            \"portuguese\": \"por_Latn\",\n",
    "            \"italian\": \"ita_Latn\",\n",
    "            \"dutch\": \"nld_Latn\",\n",
    "            \"swedish\": \"swe_Latn\",\n",
    "            \"korean\": \"kor_Hang\",\n",
    "            \"turkish\": \"tur_Latn\",\n",
    "            \"hindi\": \"hin_Deva\",\n",
    "            \"bengali\": \"ben_Beng\",\n",
    "            \"urdu\": \"urd_Arab\",\n",
    "            \"persian\": \"fas_Arab\",\n",
    "            \"thai\": \"tha_Thai\",\n",
    "            \"vietnamese\": \"vie_Latn\",\n",
    "            \"indonesian\": \"ind_Latn\",\n",
    "            \"malay\": \"zsm_Latn\",\n",
    "            \"polish\": \"pol_Latn\",\n",
    "            \"ukrainian\": \"ukr_Cyrl\",\n",
    "            \"greek\": \"ell_Grek\",\n",
    "            \"czech\": \"ces_Latn\",\n",
    "            \"finnish\": \"fin_Latn\",\n",
    "            \"hungarian\": \"hun_Latn\"\n",
    "        }\n",
    "\n",
    "    def get_language_code(self, language: str) -> str:\n",
    "        \"\"\"Get the NLLB language code for a given language name.\"\"\"\n",
    "\n",
    "        language = language.lower().strip()\n",
    "        if language in self.language_code_map:\n",
    "            return self.language_code_map[language]\n",
    "        else:\n",
    "            # If language not found, attempt to find partial matches\n",
    "            for lang, code in self.language_code_map.items():\n",
    "                if language in lang or lang in language:\n",
    "                    return code\n",
    "\n",
    "            # Default to English if no match found\n",
    "            print(f\"Warning: Language '{language}' not found in mapping. Using English as default.\\n\")\n",
    "            return self.language_code_map[\"english\"]\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        \"\"\"Detect the language of the text.\"\"\"\n",
    "        try:\n",
    "            # Import fasttext for language detection\n",
    "            import fasttext\n",
    "\n",
    "            # Check if the model exists and download if needed\n",
    "            model_path = \"lid.176.bin\"\n",
    "            if not os.path.exists(model_path):\n",
    "                print(\"-----Downloading fastText language detection model...-----\\n\")\n",
    "                import urllib.request\n",
    "                url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "                urllib.request.urlretrieve(url, model_path)\n",
    "\n",
    "            # Load the model\n",
    "            model = fasttext.load_model(model_path)\n",
    "\n",
    "            # Detect language\n",
    "            prediction = model.predict(text.replace(\"\\n\", \" \"))\n",
    "\n",
    "            # Extract language code and map to NLLB code\n",
    "            detected_lang = prediction[0][0].replace(\"__label__\", \"\")\n",
    "\n",
    "            # Map ISO code to our language name\n",
    "            iso_to_name = {\n",
    "                \"en\": \"english\",\n",
    "                \"ar\": \"arabic\",\n",
    "                \"fr\": \"french\",\n",
    "                \"es\": \"spanish\",\n",
    "                \"de\": \"german\",\n",
    "                \"zh\": \"chinese\",\n",
    "                \"ja\": \"japanese\",\n",
    "                \"ru\": \"russian\",\n",
    "                \"pt\": \"portuguese\",\n",
    "                \"it\": \"italian\",\n",
    "                \"nl\": \"dutch\",\n",
    "                \"sv\": \"swedish\",\n",
    "                \"ko\": \"korean\",\n",
    "                \"tr\": \"turkish\",\n",
    "                \"hi\": \"hindi\",\n",
    "                \"bn\": \"bengali\",\n",
    "                \"ur\": \"urdu\",\n",
    "                \"fa\": \"persian\",\n",
    "                \"th\": \"thai\",\n",
    "                \"vi\": \"vietnamese\",\n",
    "                \"id\": \"indonesian\",\n",
    "                \"ms\": \"malay\",\n",
    "                \"pl\": \"polish\",\n",
    "                \"uk\": \"ukrainian\",\n",
    "                \"el\": \"greek\",\n",
    "                \"cs\": \"czech\",\n",
    "                \"fi\": \"finnish\",\n",
    "                \"hu\": \"hungarian\"\n",
    "            }\n",
    "\n",
    "            if detected_lang in iso_to_name:\n",
    "                return self.get_language_code(iso_to_name[detected_lang])\n",
    "            else:\n",
    "                print(f\"Warning: Detected language '{detected_lang}' not mapped to NLLB code. Using English.\\n\")\n",
    "                return self.language_code_map[\"english\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in language detection: {str(e)}\")\n",
    "            print(\"Falling back to English\")\n",
    "            return self.language_code_map[\"english\"]\n",
    "\n",
    "    def _split_into_chunks(self, text: str, max_length: int = 512) -> list:\n",
    "        \"\"\"Split text into chunks for translation, preserving structure.\"\"\"\n",
    "\n",
    "        # If text is shorter than max_length, return it as a single chunk\n",
    "        if len(self.tokenizer.encode(text)) <= max_length:\n",
    "            return [text]\n",
    "\n",
    "        # First, split by paragraphs\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # If paragraph is too long, split it by sentences\n",
    "            if len(self.tokenizer.encode(paragraph)) > max_length:\n",
    "                sentences = self._split_into_sentences(paragraph)\n",
    "                for sentence in sentences:\n",
    "                    # If adding this sentence would make the chunk too long, start a new chunk\n",
    "                    if len(self.tokenizer.encode(current_chunk + sentence)) > max_length and current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                        current_chunk = sentence\n",
    "                    else:\n",
    "                        current_chunk += sentence\n",
    "            else:\n",
    "                # If adding this paragraph would make the chunk too long, start a new chunk\n",
    "                if len(self.tokenizer.encode(current_chunk + paragraph + \"\\n\\n\")) > max_length and current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = paragraph + \"\\n\\n\"\n",
    "                else:\n",
    "                    current_chunk += paragraph + \"\\n\\n\"\n",
    "\n",
    "        # Add the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> list:\n",
    "        \"\"\"Split text into sentences.\"\"\"\n",
    "\n",
    "        # Simple sentence splitting - enhance this for better results\n",
    "        import re\n",
    "        sentence_endings = r'(?<=[.!?])\\s+'\n",
    "        sentences = re.split(sentence_endings, text)\n",
    "        return [s + \". \" if not s.endswith(('.', '!', '?')) else s + \" \" for s in sentences if s]\n",
    "\n",
    "    def _preserve_formatting(self, original: str, translated: str) -> str:\n",
    "        \"\"\"Attempt to preserve formatting from original text in the translated text.\"\"\"\n",
    "\n",
    "        if not self.preserve_formatting:\n",
    "            return translated\n",
    "\n",
    "        # Preserve paragraph breaks\n",
    "        if \"\\n\\n\" in original and \"\\n\\n\" not in translated:\n",
    "            # Count paragraphs in original\n",
    "            orig_paragraphs = original.split(\"\\n\\n\")\n",
    "            if len(orig_paragraphs) > 1:\n",
    "                # Try to split translated text into similar number of paragraphs\n",
    "                trans_sentences = self._split_into_sentences(translated)\n",
    "                sentences_per_paragraph = max(1, len(trans_sentences) // len(orig_paragraphs))\n",
    "\n",
    "                new_translated = \"\"\n",
    "                for i in range(0, len(trans_sentences), sentences_per_paragraph):\n",
    "                    paragraph = \"\".join(trans_sentences[i:i+sentences_per_paragraph])\n",
    "                    new_translated += paragraph.strip() + \"\\n\\n\"\n",
    "\n",
    "                translated = new_translated.strip()\n",
    "\n",
    "        # Preserve bullet points and numbering\n",
    "        bullet_pattern = r'^\\s*[-•*]\\s+'\n",
    "        number_pattern = r'^\\s*\\d+[.)]?\\s+'\n",
    "\n",
    "        if re.search(bullet_pattern, original, re.MULTILINE) and not re.search(bullet_pattern, translated, re.MULTILINE):\n",
    "            # Original has bullet points but translated doesn't\n",
    "            orig_lines = original.split('\\n')\n",
    "            trans_lines = translated.split('\\n')\n",
    "\n",
    "            if len(orig_lines) == len(trans_lines):\n",
    "                # Match line by line\n",
    "                for i in range(len(orig_lines)):\n",
    "                    if re.match(bullet_pattern, orig_lines[i]):\n",
    "                        bullet = re.match(bullet_pattern, orig_lines[i]).group(0)\n",
    "                        trans_lines[i] = bullet + trans_lines[i].lstrip()\n",
    "\n",
    "                translated = '\\n'.join(trans_lines)\n",
    "\n",
    "        # Preserve tables (simple implementation)\n",
    "        if '|' in original and '|' not in translated:\n",
    "            # Try to identify table structures and preserve them\n",
    "            table_rows = re.findall(r'^\\s*\\|.*\\|\\s*$', original, re.MULTILINE)\n",
    "            if table_rows:\n",
    "                # Extract table content and structure\n",
    "                table_content = {}\n",
    "                for i, row in enumerate(table_rows):\n",
    "                    cells = [cell.strip() for cell in row.split('|')[1:-1]]\n",
    "                    table_content[i] = cells\n",
    "\n",
    "                # Translate table content while preserving structure\n",
    "                translated_table = {}\n",
    "                for row_idx, cells in table_content.items():\n",
    "                    translated_cells = []\n",
    "                    for cell in cells:\n",
    "                        if cell and not all(c in '- :' for c in cell):  # Skip separator rows\n",
    "                            translated_cell = self.translate(cell, target_language=\"current\")\n",
    "                            translated_cells.append(translated_cell)\n",
    "                        else:\n",
    "                            translated_cells.append(cell)\n",
    "                    translated_table[row_idx] = translated_cells\n",
    "\n",
    "                # Replace original table with translated one\n",
    "                for i, row in enumerate(table_rows):\n",
    "                    if i in translated_table:\n",
    "                        new_row = '| ' + ' | '.join(translated_table[i]) + ' |'\n",
    "                        translated = translated.replace(row, new_row)\n",
    "\n",
    "        # Preserve code blocks\n",
    "        code_block_pattern = r'```(?:\\w+)?\\n[\\s\\S]*?\\n```'\n",
    "        code_blocks = re.findall(code_block_pattern, original)\n",
    "        if code_blocks:\n",
    "            for code_block in code_blocks:\n",
    "                # Don't translate code blocks, keep them as is\n",
    "                if code_block in original and code_block not in translated:\n",
    "                    translated = translated + \"\\n\\n\" + code_block\n",
    "\n",
    "        return translated\n",
    "\n",
    "    @measure_token_processing(\"Translation\")\n",
    "    def translate(self, text: str, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate text to the target language.\"\"\"\n",
    "\n",
    "        if not text.strip():\n",
    "            return text\n",
    "\n",
    "        if self.model is None:\n",
    "            return \"Translation model not loaded properly.\"\n",
    "\n",
    "        try:\n",
    "            # Special case for \"current\" language target - just return original text\n",
    "            if target_language.lower() == \"current\":\n",
    "                return text\n",
    "\n",
    "            # Get language codes\n",
    "            target_lang_code = self.get_language_code(target_language)\n",
    "\n",
    "            # Auto-detect source language if not provided\n",
    "            if not source_language:\n",
    "                source_lang_code = self.detect_language(text)\n",
    "                print(f\"-----Detected source language code: {source_lang_code}-----\\n\")\n",
    "            else:\n",
    "                source_lang_code = self.get_language_code(source_language)\n",
    "\n",
    "            # If source and target languages are the same, return original text\n",
    "            if source_lang_code == target_lang_code:\n",
    "                return text\n",
    "\n",
    "            # Split text into manageable chunks\n",
    "            chunks = self._split_into_chunks(text)\n",
    "            translated_chunks = []\n",
    "\n",
    "            # Translate each chunk\n",
    "            for chunk in chunks:\n",
    "                if not chunk.strip():\n",
    "                    translated_chunks.append(chunk)\n",
    "                    continue\n",
    "\n",
    "                # Prepare inputs for the model\n",
    "                if \"nllb\" in self.model_name.lower():\n",
    "                    # NLLB model\n",
    "                    self.tokenizer.src_lang = source_lang_code\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        generated_tokens = self.model.generate(\n",
    "                            **inputs,\n",
    "                            forced_bos_token_id=self.tokenizer.convert_tokens_to_ids(target_lang_code),\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.batch_decode(\n",
    "                        generated_tokens, skip_special_tokens=True\n",
    "                    )[0]\n",
    "\n",
    "                elif \"m2m100\" in self.model_name.lower():\n",
    "                    # M2M100 model\n",
    "                    self.tokenizer.src_lang = source_lang_code.split('_')[0]\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "                    self.tokenizer.tgt_lang = target_lang_code.split('_')[0]\n",
    "                    with torch.no_grad():\n",
    "                        generated_tokens = self.model.generate(\n",
    "                            **inputs,\n",
    "                            forced_bos_token_id=self.tokenizer.get_lang_id(target_lang_code.split('_')[0]),\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.batch_decode(\n",
    "                        generated_tokens, skip_special_tokens=True\n",
    "                    )[0]\n",
    "\n",
    "                else:\n",
    "                    # Generic approach for other models\n",
    "                    inputs = self.tokenizer(chunk, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model.generate(\n",
    "                            **inputs,\n",
    "                            max_length=1024,\n",
    "                            num_beams=5,\n",
    "                            length_penalty=1.0\n",
    "                        )\n",
    "\n",
    "                    translated_chunk = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "                # Preserve formatting\n",
    "                if self.preserve_formatting:\n",
    "                    translated_chunk = self._preserve_formatting(chunk, translated_chunk)\n",
    "\n",
    "                translated_chunks.append(translated_chunk)\n",
    "\n",
    "            # Combine translated chunks\n",
    "            full_translation = \" \".join(translated_chunks)\n",
    "\n",
    "            # Clean up any artifacts from the chunking process\n",
    "            full_translation = full_translation.replace(\"  \", \" \")\n",
    "            full_translation = re.sub(r'\\s+\\.', '.', full_translation)\n",
    "            full_translation = re.sub(r'\\s+,', ',', full_translation)\n",
    "\n",
    "            return full_translation\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in translation: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return f\"Translation error: {str(e)}\"\n",
    "\n",
    "    def translate_file(self, file_path: str, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate a file to the target language.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Determine file type and handle accordingly\n",
    "            file_path = Path(file_path)\n",
    "            file_extension = file_path.suffix.lower()\n",
    "\n",
    "            # Create output file path\n",
    "            output_dir = file_path.parent / \"translated\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            output_file = output_dir / f\"{file_path.stem}_{target_language}{file_extension}\"\n",
    "\n",
    "            if file_extension in ['.docx', '.doc']:\n",
    "                # Word document - use python-docx\n",
    "                return self._translate_docx(file_path, output_file, target_language, source_language)\n",
    "\n",
    "            elif file_extension == '.pdf':\n",
    "                # PDF - extract text, translate, create new PDF\n",
    "                return self._translate_pdf(file_path, output_file, target_language, source_language)\n",
    "\n",
    "            else:\n",
    "                # Default to treating as plain text\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                translated_content = self.translate(content, target_language, source_language)\n",
    "\n",
    "            # Write translated content to output file\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(translated_content)\n",
    "\n",
    "            return str(output_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating file: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            return f\"File translation error: {str(e)}\"\n",
    "\n",
    "    def _translate_json(self, data, target_language: str, source_language: str = None):\n",
    "        \"\"\"Recursively translate values in a JSON object.\"\"\"\n",
    "\n",
    "        if isinstance(data, str):\n",
    "            return self.translate(data, target_language, source_language)\n",
    "        elif isinstance(data, list):\n",
    "            return [self._translate_json(item, target_language, source_language) for item in data]\n",
    "        elif isinstance(data, dict):\n",
    "            return {k: self._translate_json(v, target_language, source_language) for k, v in data.items()}\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "\n",
    "    def _translate_docx(self, input_path, output_path, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"Translate a Word document.\"\"\"\n",
    "\n",
    "        try:\n",
    "            import docx\n",
    "\n",
    "            # Load the document\n",
    "            doc = docx.Document(input_path)\n",
    "\n",
    "            # Translate each paragraph\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    translated_text = self.translate(para.text, target_language, source_language)\n",
    "\n",
    "                    # Preserve runs with their formatting\n",
    "                    if len(para.runs) > 1:\n",
    "                        # This is an approximation and won't be perfect\n",
    "                        words = translated_text.split()\n",
    "                        runs_text = [run.text for run in para.runs if run.text.strip()]\n",
    "\n",
    "                        # Calculate distribution\n",
    "                        total_original_length = sum(len(text) for text in runs_text)\n",
    "                        words_per_run = []\n",
    "\n",
    "                        for run_text in runs_text:\n",
    "                            proportion = len(run_text) / total_original_length if total_original_length > 0 else 0\n",
    "                            words_for_run = max(1, int(proportion * len(words)))\n",
    "                            words_per_run.append(words_for_run)\n",
    "\n",
    "                        # Adjust to make sure all words are distributed\n",
    "                        while sum(words_per_run) < len(words):\n",
    "                            words_per_run[-1] += 1\n",
    "\n",
    "                        # Apply to runs\n",
    "                        word_index = 0\n",
    "                        for i, run in enumerate(para.runs):\n",
    "                            if i < len(words_per_run):\n",
    "                                run_words = words[word_index:word_index + words_per_run[i]]\n",
    "                                run.text = \" \".join(run_words)\n",
    "                                word_index += words_per_run[i]\n",
    "                            else:\n",
    "                                run.text = \"\"\n",
    "                    else:\n",
    "                        # Simple case - just replace text\n",
    "                        para.text = translated_text\n",
    "\n",
    "            # Translate table contents\n",
    "            for table in doc.tables:\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        for para in cell.paragraphs:\n",
    "                            if para.text.strip():\n",
    "                                para.text = self.translate(para.text, target_language, source_language)\n",
    "\n",
    "            # Save the document\n",
    "            doc.save(output_path)\n",
    "\n",
    "            return str(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating Word document: {str(e)}\")\n",
    "            return f\"Word document translation error: {str(e)}\"\n",
    "\n",
    "\n",
    "    def _translate_pdf(self, input_path, output_path, target_language: str, source_language: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from PDF, translate it page by page, and save only as text files.\n",
    "        Returns the path to the complete translated text file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import PyPDF2\n",
    "            import os\n",
    "            from pathlib import Path\n",
    "\n",
    "            # Convert Path objects to strings\n",
    "            input_path_str = str(input_path)\n",
    "\n",
    "            # Create output directory and path for text file\n",
    "            output_dir = os.path.dirname(str(output_path))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Create text file path (replace .pdf with .txt if needed)\n",
    "            output_txt = str(output_path).replace('.pdf', '.txt')\n",
    "\n",
    "            # Create directory for individual pages\n",
    "            base_filename = os.path.basename(output_txt).split('.')[0]\n",
    "            pages_dir = os.path.join(output_dir, f\"{base_filename}_pages\")\n",
    "            os.makedirs(pages_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Starting page-by-page translation of PDF: {input_path_str}\")\n",
    "\n",
    "            # Open the PDF\n",
    "            with open(input_path_str, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                total_pages = len(reader.pages)\n",
    "                print(f\"PDF has {total_pages} pages\")\n",
    "\n",
    "                all_translated_text = \"\"\n",
    "\n",
    "                # Process each page individually\n",
    "                for page_num in range(total_pages):\n",
    "                    try:\n",
    "                        print(f\"Processing page {page_num + 1} of {total_pages}\")\n",
    "\n",
    "                        # Extract text from this page\n",
    "                        page_text = reader.pages[page_num].extract_text()\n",
    "\n",
    "                        if not page_text.strip():\n",
    "                            print(f\"Page {page_num + 1} appears to be empty or contains only images\")\n",
    "                            page_info = f\"\\n\\n--- Page {page_num + 1} (Empty or contains only images) ---\\n\\n\"\n",
    "                            all_translated_text += page_info\n",
    "\n",
    "                            # Save empty page file as a placeholder\n",
    "                            page_file = os.path.join(pages_dir, f\"page_{page_num + 1}.txt\")\n",
    "                            with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                                f.write(page_info)\n",
    "                            continue\n",
    "\n",
    "                        # Translate this page's text\n",
    "                        translated_page = self.translate(page_text, target_language, source_language)\n",
    "\n",
    "                        # Add page information\n",
    "                        page_content = f\"\\n\\n--- Page {page_num + 1} ---\\n\\n{translated_page}\"\n",
    "                        all_translated_text += page_content\n",
    "\n",
    "                        # Save individual page translation\n",
    "                        page_file = os.path.join(pages_dir, f\"page_{page_num + 1}.txt\")\n",
    "                        with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(page_content)\n",
    "\n",
    "                        print(f\"Saved translated page {page_num + 1} to {page_file}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing page {page_num + 1}: {str(e)}\")\n",
    "                        error_message = f\"\\n\\n--- Error on Page {page_num + 1}: {str(e)} ---\\n\\n\"\n",
    "                        all_translated_text += error_message\n",
    "\n",
    "                        # Save error information\n",
    "                        page_file = os.path.join(pages_dir, f\"page_{page_num + 1}_error.txt\")\n",
    "                        with open(page_file, 'w', encoding='utf-8') as f:\n",
    "                            f.write(error_message)\n",
    "\n",
    "            # Save the complete text version\n",
    "            with open(output_txt, 'w', encoding='utf-8') as f:\n",
    "                f.write(all_translated_text)\n",
    "            print(f\"Saved complete translated text to {output_txt}\")\n",
    "            print(f\"Individual translated pages saved to {pages_dir}\")\n",
    "\n",
    "            return output_txt\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error translating PDF: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return f\"PDF translation error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "aa47982f7a2d43729158c2cb846d2b77",
      "7469f3fc345649d7aa4bd83cff08790e",
      "95a7417201c04adeab538d238581bb7e",
      "d84dace805004830a2f0a0ba155f8e8c",
      "d5508b88d7e84ca9b0101ca0f9e23ad2",
      "76bcc84e3ecb4e3e8c75607f8dd08048",
      "4e19e3ab1a714bb598bac3a6cdee6276",
      "7ed982f2a18e459b8637df68209a2a90",
      "c294a41ff11b4ef79c80a8b4a78f5664",
      "ae85daefeadb48e48d2c14a5074cb995",
      "45db328b2ffc4a7888d5bdeb6607f5a9",
      "cc9ec16b09a4433b9949c8684750003e",
      "9704fb4980004054a1588c5696e1bb66",
      "a62e07525d964d5b89f692c1e1c90169",
      "5f9e90d96cd142f2a68792a351ae5746",
      "12528d6a8be042ecb01e5082bed56564",
      "3e74743fbd0149fdbdf006f104b412c8",
      "f94dfda854274b66bda799709c40d80d",
      "21134b169643475b8767f5eaf8a41479",
      "0fbe335acdc94d549cc1089aa8aafbd2",
      "cbba973fb80c43298aa3aa6ccac50ab4",
      "795c4d7e24e84357969f4ca9d9e26160",
      "1f6cb79ee68c4530ba61cdbb492db330",
      "3fb959dfbdf34e0aa120a4a42e980c62",
      "afbc48629935452ab73b4800d1ff39a8",
      "e031270c473448d3acd703c4db91ac53",
      "64d470b3cab14d7d89918b19020882a2",
      "48117d6196fd4dc68bfc5ff34e8ed8dd",
      "3050cd92fc234cc19467c8049e54e1b8",
      "6c74394cd8e34fb681050277449dd7e3",
      "4041feaff156418baabd6f5ce0aa2f55",
      "423a2af65ea544208cfc5fa5cc623e4e",
      "46aea010b6d645faace1f935b7ba8876",
      "6c29aecf01024f77b14068a4a36c806c",
      "fa8c7ee4349e4aeda1da881d12ba187c",
      "39ac215717b2424caed9c09827b28170",
      "10c7291c3fb649bda4a8abf77fbeb39a",
      "ad6a7b7235ce4ba3bc82e65a16d3cc19",
      "57dc84debbec4d68a89165833564bd3b",
      "ad314e87b70a488784148aef2f4d8c86",
      "ded1dd921ccc42268f363a6a2ed1976c",
      "d84149e7b0cd4b798d7970e3e102daeb",
      "dd75cdce4ff143098ada7cde8d7e20f8",
      "76ebae3315b94618a01219fed6077ef2",
      "f15a235d4f16444399ad8c3354bb3005",
      "3327f155737f4566bef4fd87837f2544",
      "dde3310f6972448c9712589daa8e8bad",
      "fa00b1a4fdda4f63962216900d122479",
      "9d7c311d9ce745048d773d9ee9577b47",
      "bd181799e628432dacb8c3135ca2e503",
      "3a08afd652ed40c3a1fea566f950804a",
      "93b46b06618047dba17240cf6e5d767d",
      "83eeeb9a929847cbb71a03e5611df63c",
      "66bb5e731aa34211870a13303d0f2192",
      "106328586b49463f8ab8e1c6046de72c",
      "3579531daf8c4d989eaa26984068ec81",
      "df5b1fe5ce844051abf147d4dc8d7089",
      "f499fcb40a5b4783aa1d552d2a151089",
      "8a05c8a6d95845c8b5f16bc9ad36222e",
      "909bcb3bf38d4c5184f3a18f198e3e2d",
      "1fad12a7dfb743f39835d964144c8ec5",
      "2c7cc874fcdf41dc995668a1b408228b",
      "f327a191705840de9177486fdbf0a922",
      "f20a7e86620c44d4aa54e056fe680d30",
      "58cdc30e19a149638394f86df9227c72",
      "c5a577b68ce0461ab89b5d7c81ce22a8",
      "af17b673107d437796149fd2378ac08e",
      "b3dabc286a2d4099a71172cb41431aca",
      "b38718f90fdd4eb1850c5c574702abfe",
      "b015e8536b9e4ae4a9609e28a25dea92",
      "4f375c3b475c4609b182e2a42df01594",
      "bfd95242d18b4f7fb06e0713e711692c",
      "b971252dbafb4a88972b2924b4b305ff",
      "c1145a0cbb5645d4a97ee899ecf24c8a",
      "45bcdf18650b40478759a2718753095b",
      "1b76b9fc7b474b348e8cfb7fcd8fcbf2",
      "6f1d6466990e4d5ba5a563a6d8c07117",
      "9d4848533a884844b341b34d335d892e"
     ]
    },
    "id": "3SxymNC62gqR",
    "outputId": "b5a90215-6c09-49d4-9f75-5506632d8a2c"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Initializing translation service with model facebook/nllb-200-distilled-600M on cuda-----\n",
      "\n",
      "-----Loading translation model: facebook/nllb-200-distilled-600M-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa47982f7a2d43729158c2cb846d2b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9ec16b09a4433b9949c8684750003e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6cb79ee68c4530ba61cdbb492db330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c29aecf01024f77b14068a4a36c806c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15a235d4f16444399ad8c3354bb3005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3579531daf8c4d989eaa26984068ec81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af17b673107d437796149fd2378ac08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dabc286a2d4099a71172cb41431aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translator = TranslationService(\n",
    "    model_name = \"facebook/nllb-200-distilled-600M\",\n",
    "    device = \"auto\",\n",
    "    preserve_formatting = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lhap_yh9bQJ6",
    "outputId": "0d427b3b-5394-4892-a2d6-e3bcc711a5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 2.19s (7.30 tokens/sec)\n",
      "\n",
      "   Total (in+out): 33 tokens at 15.05 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: Le renard brun rapide saute sur le chien paresseux.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating a simple sentence from English to French\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT_RgmyVc_U7",
    "outputId": "43d137cd-967d-4fa2-9e22-a745029f5a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 1.04s (15.42 tokens/sec)\n",
      "\n",
      "   Total (in+out): 35 tokens at 33.73 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: الثعلب البني السريع يقفز فوق الكلب الكسول.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating to Arabic\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"ar\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GiFrJ9lg3ff",
    "outputId": "f9b11456-605c-4c5b-d589-58b0a8bd21fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 16 tokens processed in 2.28s (7.03 tokens/sec)\n",
      "\n",
      "   Total (in+out): 35 tokens at 15.38 tokens/sec\n",
      "\n",
      "\n",
      "Translated text: Быстрая коричневая лиса прыгает над ленивой собакой.\n"
     ]
    }
   ],
   "source": [
    "# Let's try translating to Russian\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"ru\"\n",
    "\n",
    "translated_text = translator.translate(text, target_language, source_language)\n",
    "print(\"Translated text:\", translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWkbfvcraaF5"
   },
   "source": [
    "Instead of directly translating entire files, let's perform the translation on the chunks created during the chunking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jwWYF2Y34qS",
    "outputId": "8e850f6c-fe2d-4130-c911-bf949b5ec540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/chunked_data.zip\n",
      "   creating: content/chunked_data/\n",
      "  inflating: content/chunked_data/The-Alchemist_chunks.json  \n",
      "  inflating: content/chunked_data/Ocean_ecogeochemistry_A_review_chunks.json  \n",
      "  inflating: content/chunked_data/Stats_chunks.json  \n",
      "  inflating: content/chunked_data/new-approaches-and-procedures-for-cancer-treatment_chunks.json  \n",
      "  inflating: content/chunked_data/all_chunks.json  \n",
      "  inflating: content/chunked_data/The_Plan_of_the_Giza_Pyramids_chunks.json  \n",
      "  inflating: content/chunked_data/M.Sc. Applied Psychology_chunks.json  \n",
      "  inflating: content/chunked_data/Dataset summaries and citations_chunks.json  \n"
     ]
    }
   ],
   "source": [
    "# Unzipping chunked data\n",
    "!unzip /content/chunked_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84OudBQ937Ig"
   },
   "outputs": [],
   "source": [
    "!mkdir chunked_data\n",
    "!mv /content/content/chunked_data/* /content/chunked_data\n",
    "!rm -rf /content/content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuRJT_PGdeCR"
   },
   "source": [
    "### Loading and Grouping Document Chunks by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kppazHy4Ug6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chunks_dir = \"/content/chunked_data\"\n",
    "def load_chunks(filename: str = \"all_chunks.json\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load document chunks from a JSON file.\"\"\"\n",
    "    filepath = os.path.join(chunks_dir, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks from {filepath}\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunks from {filepath}: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFUJarnu4EqA"
   },
   "outputs": [],
   "source": [
    "def group_chunks_by_source(chunks: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"Group chunks by their source document.\"\"\"\n",
    "    grouped = {}\n",
    "    for chunk in chunks:\n",
    "        source = chunk.get(\"source\", \"unknown\")\n",
    "        if source not in grouped:\n",
    "            grouped[source] = []\n",
    "        grouped[source].append(chunk)\n",
    "\n",
    "    # Sort chunks by chunk number within each source\n",
    "    for source in grouped:\n",
    "        grouped[source].sort(key=lambda x: x.get(\"chunk_number\", 0))\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1A708Hy4M5u",
    "outputId": "5f111488-8df2-474f-d21d-ebce7bf01bed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 599 chunks from /content/chunked_data/all_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# Group chunks by source\n",
    "chunks = load_chunks()\n",
    "grouped_chunks = group_chunks_by_source(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6po55AC4hp4",
    "outputId": "0ebd3162-0cce-4980-9864-7361c4713e0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['new-approaches-and-procedures-for-cancer-treatment.pdf', 'The_Plan_of_the_Giza_Pyramids.pdf', 'The-Alchemist.pdf', 'Stats.docx', 'Ocean_ecogeochemistry_A_review.pdf', 'Dataset summaries and citations.docx', 'M.Sc. Applied Psychology.docx'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_chunks.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq4SoZ5xdvki"
   },
   "source": [
    "Now let's try translation from 'Stats.docx' document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "pJvaGTvg4kRQ",
    "outputId": "f5e1271b-d8b1-4675-f0f8-6c2da9ae664f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Introduction The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs. The file provides a list of envelope and non-envelope components (e.g., Windows, Water Heaters) and any associated classes within those components (e.g., Low Emissivity, Electric Instantaneous).'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_chunks['Stats.docx'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVEvdYP04xJZ",
    "outputId": "f049d6f2-fc36-4994-d94d-87859e24a948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Downloading fastText language detection model...-----\n",
      "\n",
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 8.98s (14.93 tokens/sec)\n",
      "   Total (in+out): 282 tokens at 31.41 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to Arabic\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "ricumFXg-UAx",
    "outputId": "d6fe2783-7eb9-4999-9aa0-421b1969a085"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'مقدمة الغرض من ملف القراءة الآلية (the file) هو توفير معايير التراجع والقاطع للمكونات المختلفة لحساب تقديرات أسعار المواد المنخفضة والمتوسطة والعالية (10 و50 و90 مئوية) ومضاعفات العمالة / إضافات لتقدير تكاليف المشروعات الجديدة في البناء والتعديل. يقدم الملف قائمة بالمكونات الغلافية وغير الغلافية (مثل Windows و Heaters Water) وأي فئات مرتبطة ضمن تلك المكونات (مثل Low Emissivity و Electric Instantaneous).'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dzn9pr0-d4h",
    "outputId": "e5e865fd-f007-4c47-b82e-1fcba04bdaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 10.07s (13.31 tokens/sec)\n",
      "   Total (in+out): 236 tokens at 23.44 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to French\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "TXCiWT_I-YrA",
    "outputId": "5f3635e1-bd98-41c9-c3c2-55b82659382e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Introduction Le but du fichier Machine Readable (the file) est de fournir des coefficients de régression et des interceptions pour différents composants pour calculer les estimations basses, moyennes et élevées (10e, 50e et 90e percentiles) des prix des matériaux et des multiplicateurs de main-d'œuvre / add-ons pour estimer les coûts de nouveaux projets de construction et de rénovation.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqglZ75ABH46",
    "outputId": "b310d700-ac06-4a66-8b02-a43da8727576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Detected source language code: eng_Latn-----\n",
      "\n",
      "⏱️ Translation: 134 tokens processed in 7.91s (16.95 tokens/sec)\n",
      "   Total (in+out): 267 tokens at 33.77 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "# Translation to Japanese\n",
    "translation = translator.translate(grouped_chunks['Stats.docx'][0]['text'], target_language=\"ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "IS_UXFOHBLGx",
    "outputId": "b72f0c67-46d9-4bf6-ffab-ddab3281e0ce"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'導入 マシンリーダブルファイル (the file) の目的は,低,中,高 (10 番目の,50 番目の,90 番目の百分点) の材料価格推定と労働倍数/アドオンを計算するために,異なるコンポーネントの回帰系数と傍受を提供することです.このファイルは,新しい建設と改装プロジェクトのコストを推定するために,包装および非包装コンポーネント (例えば,Windows,Water Heaters) とそれらのコンポーネント内の関連クラス (例えば,Low Emissivity, Electric Instantaneous) のリストを提供します.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw82HlOXePRM"
   },
   "source": [
    "We can translate text into a wide range of languages using the current model. The supported languages are defined in a dictionary `iso_to_name` that maps language names to their corresponding NLLB language codes. This dictionary is easily extendable, allowing us to add support for even more languages as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnavI-ZCeyHy"
   },
   "source": [
    "Now let's try to tranlsate entire 'Stats.docx' document but in json chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkH1Ukk9CaYr",
    "outputId": "b016ab21-2e8d-41b3-b5a2-52cdcef371ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 7 tokens processed in 0.49s (14.30 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 32.68 tokens/sec\n",
      "⏱️ Translation: 134 tokens processed in 7.91s (16.93 tokens/sec)\n",
      "   Total (in+out): 282 tokens at 35.63 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.39s (7.62 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 17.78 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.27 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 26.59 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.68s (10.24 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 23.42 tokens/sec\n",
      "⏱️ Translation: 476 tokens processed in 56.37s (8.44 tokens/sec)\n",
      "   Total (in+out): 1497 tokens at 26.56 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.68 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.91 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.59 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.71 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.38s (15.85 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 34.35 tokens/sec\n",
      "⏱️ Translation: 119 tokens processed in 14.56s (8.17 tokens/sec)\n",
      "   Total (in+out): 361 tokens at 24.79 tokens/sec\n",
      "⏱️ Translation: 288 tokens processed in 59.33s (4.85 tokens/sec)\n",
      "   Total (in+out): 1286 tokens at 21.68 tokens/sec\n",
      "⏱️ Translation: 289 tokens processed in 63.21s (4.57 tokens/sec)\n",
      "   Total (in+out): 1022 tokens at 16.17 tokens/sec\n",
      "⏱️ Translation: 135 tokens processed in 11.98s (11.27 tokens/sec)\n",
      "   Total (in+out): 327 tokens at 27.30 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.48s (14.49 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 33.13 tokens/sec\n",
      "⏱️ Translation: 385 tokens processed in 36.21s (10.63 tokens/sec)\n",
      "   Total (in+out): 1004 tokens at 27.73 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.29s (10.36 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.17 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.66 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.87 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.39s (15.41 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 33.39 tokens/sec\n",
      "⏱️ Translation: 87 tokens processed in 5.37s (16.22 tokens/sec)\n",
      "   Total (in+out): 183 tokens at 34.11 tokens/sec\n",
      "⏱️ Translation: 85 tokens processed in 5.28s (16.10 tokens/sec)\n",
      "   Total (in+out): 174 tokens at 32.96 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.49s (14.38 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 32.87 tokens/sec\n",
      "⏱️ Translation: 489 tokens processed in 53.10s (9.21 tokens/sec)\n",
      "   Total (in+out): 1410 tokens at 26.55 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.71 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.99 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.40s (9.88 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 22.24 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.80 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 25.20 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.47s (12.71 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 27.54 tokens/sec\n",
      "⏱️ Translation: 187 tokens processed in 14.68s (12.74 tokens/sec)\n",
      "   Total (in+out): 442 tokens at 30.12 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.47s (14.74 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 33.69 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.45s (15.47 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 30.95 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.29s (10.39 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.25 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.50s (13.98 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 31.96 tokens/sec\n",
      "⏱️ Translation: 260 tokens processed in 23.12s (11.25 tokens/sec)\n",
      "   Total (in+out): 659 tokens at 28.50 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.29s (10.41 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.28 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.40s (14.97 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 32.43 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.49s (14.23 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 32.53 tokens/sec\n",
      "⏱️ Translation: 210 tokens processed in 13.60s (15.44 tokens/sec)\n",
      "   Total (in+out): 454 tokens at 33.38 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.28s (10.65 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 24.86 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.37s (16.06 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 34.80 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "translated_jsons = []\n",
    "for chunk in grouped_chunks['Stats.docx']:\n",
    "    translated_json_data = translator._translate_json(chunk, target_language=\"ar\", source_language=\"en\")\n",
    "    translated_jsons.append(translated_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ixigQcPB1fq",
    "outputId": "0a25df0a-faeb-41d0-f22b-52b04eb71a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [1], 'chunk_number': 0, 'text': 'Introduction The purpose of the Machine Readable file (“the file”) is to provide regression coefficients and intercepts for different components to calculate low, mid, and high (10th, 50th, and 90th percentile) material price estimates and labor multipliers/add-ons to estimate new construction and retrofit project costs. The file provides a list of envelope and non-envelope components (e.g., Windows, Water Heaters) and any associated classes within those components (e.g., Low Emissivity, Electric Instantaneous).', 'token_count': 105, 'element_types': ['title', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [1], 'chunk_number': 0, 'text': 'مقدمة الغرض من ملف القراءة الآلية (the file) هو توفير معايير التراجع والقاطع للمكونات المختلفة لحساب تقديرات أسعار المواد المنخفضة والمتوسطة والعالية (10 و50 و90 مئوية) ومضاعفات العمالة / إضافات لتقدير تكاليف المشروعات الجديدة في البناء والتعديل. يقدم الملف قائمة بالمكونات الغلافية وغير الغلافية (مثل Windows و Heaters Water) وأي فئات مرتبطة ضمن تلك المكونات (مثل Low Emissivity و Electric Instantaneous).', 'token_count': 105, 'element_types': ['العنوان', 'النص الترويجي']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [1], 'chunk_number': 1, 'text': 'Material Price Regression The Component & Class portion of the file shows the component, the class, and the output units. The “output units” column describes what the units of final output of the regression will be (e.g., 2023$, 2023$/sqft).\\n\\nComponent & Class Component Class Output Units Water Heater HP Tank w/ Gas Capping 2023$ The second and third sections of the file (Retail Price Regression) show the “Coefficient-Low”, “Coefficient-Mid”, and “Coefficient-High” values that correspond to the low, mid, and high quantile regression coefficients that are used to multiply the chosen performance metric values. These sections also contain the name and units of each performance metric and the associated unit and the lower and upper bounds of the regression. Each component is fitted via quantile regression to one (e.g., clothes dryers) or two (e.g., water heaters) applicable performance metrics. Some components, such as thermostats, have no regression analysis as there is no measurable performance metric associated with the product. The performance metrics were chosen based on market research on pricing factors and customer needs. Some components do not have any performance metrics because there were no significant component differentiators beyond the class types. The Retail Price Regression section of the file also contains the intercepts for the quantile regression equations.\\n\\nRetail Price Regression, Performance metric 1 Coefficient-Low Coefficient-Mid Coefficient-High Metric Unit Lower Bound Upper Bound 102.33 248.33 888.75 UEF Unitless 3 4.07\\n\\nRetail Price Regression, Performance metric 2 Coefficient-Low Coefficient-Mid Coefficient-High Metric Unit Lower Bound Upper Bound 28.81 19.33 8.39 Nominal volume gallons 40 80\\n\\nRetail Price Regression, Intercepts Int-Low Int-Mid Int-High 155.30 436.45 -651.90', 'token_count': 393, 'element_types': ['table', 'title', 'narrativetext'], 'tables': ['<table><tr><td>Component &amp; Class</td><td>Component &amp; Class</td><td>Component &amp; Class</td></tr><tr><td>Component</td><td>Class</td><td>Output Units</td></tr><tr><td>Water Heater</td><td>HP Tank w/ Gas Capping</td><td>2023$</td></tr></table>', '<table><tr><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td><td>Retail Price Regression, Performance metric 1</td></tr><tr><td>Coefficient-Low</td><td>Coefficient-Mid</td><td>Coefficient-High</td><td>Metric</td><td>Unit</td><td>Lower Bound</td><td>Upper Bound</td></tr><tr><td>102.33</td><td>248.33</td><td>888.75</td><td>UEF</td><td>Unitless</td><td>3</td><td>4.07</td></tr></table>', '<table><tr><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td><td>Retail Price Regression, Performance metric 2</td></tr><tr><td>Coefficient-Low</td><td>Coefficient-Mid</td><td>Coefficient-High</td><td>Metric</td><td>Unit</td><td>Lower Bound</td><td>Upper Bound</td></tr><tr><td>28.81</td><td>19.33</td><td>8.39</td><td>Nominal volume</td><td>gallons</td><td>40</td><td>80</td></tr></table>', '<table><tr><td>Retail Price Regression, Intercepts</td><td>Retail Price Regression, Intercepts</td><td>Retail Price Regression, Intercepts</td></tr><tr><td>Int-Low</td><td>Int-Mid</td><td>Int-High</td></tr><tr><td>155.30</td><td>436.45</td><td>-651.90</td></tr></table>']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [1], 'chunk_number': 1, 'text': 'عودة الأسعار المادية الجزء الثاني والثالث من الملف (عودة أسعار التجزئة) يظهر المكون والفئة ووحدات الخروج.\\n\\nعمود output units يصف ما هي وحدات الخروج النهائي للعودة (على سبيل المثال 2023$، 2023$/sqft).\\n\\nوحدات الناتج المكون والفئة المكونة ووحدات الناتج المكونة التي يتم استخدامها لضرب قيم المعايير المكونة المختارة.\\n\\nتحتوي هذه القسمات أيضًا على أسماء معايير الأداء و بعض وحدات ووحدات المكونات المكونة من المكونات التجزئة المرتبطة بالعودة إلى التجزئة.\\n\\nلا يوجد أي معايير قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية قياسية.', 'token_count': 393, 'element_types': ['الطاولة', 'العنوان', 'النص الترويجي'], 'tables': ['المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات المكونات', 'التراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 1</td><td><td>تقييم الأداء 1</td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><<td><td><td><td><td><td><td><td><td><td><<<<<td><td><td><td><<<td><<td><<td><td><td> <td><<td><<<td><<<td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>><<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>', 'التراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td></td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تراجع في أسعار التجزئة، وتقييم الأداء 2</td><td>تقييم الأداء 2</td><td><td>تقييم الأوسط</td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td><td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <td> <', 'سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة سعر التجزئة']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [2], 'chunk_number': 2, 'text': 'Labor and Installed Cost The total installed cost is calculated one of two ways depending on the component. The first method is using an installation multiplier to derive the total installed cost based on the material price. The labor multipliers are separated out by “scenario” (new construction or retrofit). Retrofit scenarios include costs for removal or other demolition of existing components. After getting the estimated material price from calculating the material price regression using the coefficients, intercepts, and chosen performance metric input values, the multiplier is used to calculate the total installed cost or cost per square foot. The material and equipment price must be calculated first in order to use the labor cost multipliers. See Example 1. The second method to calculate the total installed cost is by using adders rather than multipliers. Labor for some of the components does not scale with increasing material or equipment price and has a constant installation cost (i.e., certain types of insulation upgrades). The values are therefore added to the material price regression results to produce the total installed cost or cost per square foot. See Example 2.\\n\\nInstallation Multiplier New Construction Retrofit 1.58 3.00\\n\\nInstallation Adder New Construction Retrofit 0.83 1.0 Note for certain technologies with recent standards activity and data availability the installation costs are derived from the Technical Support Documents for the given appliance and averaged to create an installation adder. For technologies that do not have recent standards data available, installation costs were derived using a variety of available resources including RSMeans. For example, Heat Pump Water Heaters and other products covered under appliance standards rely on the updated appliance standards data to derive the installation costs.', 'token_count': 328, 'element_types': ['table', 'title', 'narrativetext'], 'tables': ['<table><tr><td>Installation Multiplier</td><td>Installation Multiplier</td></tr><tr><td>New Construction</td><td>Retrofit</td></tr><tr><td>1.58</td><td>3.00</td></tr></table>', '<table><tr><td>Installation Adder</td><td>Installation Adder</td></tr><tr><td>New Construction</td><td>Retrofit</td></tr><tr><td>0.83</td><td>1.0</td></tr></table>']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [2], 'chunk_number': 2, 'text': 'يتم حساب إجمالي التكلفة التثبيتية من خلال إحدى الطرق الثنائية اعتمادا على المكون. يتم استخدام الطريقة الأولى مضاعف التثبيت لتحصل على إجمالي التكلفة التثبيتية بناءً على سعر المادة. يتم فصل مضاعف العمل عن طريق سيناريو (بناء جديد أو تعديل).\\n\\nتشمل سيناريوهات التعديل تكاليف لإزالة أو هدم آخر للمكونات الحالية. بعد الحصول على التكلفة المادية المقدرة من حساب تراجع سعر المادة باستخدام المعاملات، والقطع، والقيمات المقايية للأداء المحددة، يستخدم المضاعف لحساب إجمالي التكلفة التثبيتية أو التكلفة التثبيتية لكل قدم مربع. يجب حساب متوسط سعر المواد والمعدات أولاً من أجل استخدام مضاعف التكلفة العاملة.\\n\\nانظر مثال 1. يتم حساب الطريقة الثانية لتعديل المعايير الإجمالية من قبل المواصفات الحديثة بدلاً من المواصفات المتقدمة. لا يتم تحديد بيانات بيانات عن تكلفة المعدات المادية مع تكاليف المعدات المادية المادية مع تكاليف المتزايدة أو تكاليف التكلفة المعدة المعدة المعدة المعدة المعدة المعدة المعدة.\\n\\nولذلك لا يتم استخدام تقنيات التعديل المعدة المعدة المعدة المعدة المعدة المعدة المعدة للتث المعدة المعدة المعدة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المتاحة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المتاحة للمعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة للمعدة المعدة المعدة المعدة المعدة للمعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة.', 'token_count': 328, 'element_types': ['الطاولة', 'العنوان', 'النص الترويجي'], 'tables': ['طاولة><tr><td>تكثيف التثبيت</td><td>تكثيف التثبيت</td></tr><tr><td><td>بناء جديد</td><td><td>تعديل</td></tr><tr><td><td>1.58</td><td>3.00</td></tr></table>', 'طاولة><tr><td>إضافة التثبيت</td><td>إضافة التثبيت</td></tr><tr><td>بناء جديد</td><td>تجديد</td></tr><tr><td><td>0.83</td><td>1.0</td></tr></table>']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [2, 3], 'chunk_number': 3, 'text': 'Additional Data The last section of the file contains additional data not directly within the calculation of each component and product class. These include the expected lifetime (in years) of the component, cost variation considerations, a list of data sources used in the analysis for each component (using a numbering format), and a qualitative confidence rating of the data. For cost variation considerations, the following implications may have additional impacts on pricing for each component. Prevailing local wages Drive time Access Presence/condition/type of existing insulation Existing construction and materials Moisture issues present Condition of existing flue Need for condensate line/drain Need to bring in combustion air Condition of existing electrical system Presence of hazardous materials Nature/size of leaks Extent of preparation Each regression was given a confidence rating in the categories of sample size (SS), median (R2), and source diversity, to qualify how robust the data and corresponding regressions are. If a dataset had a sample size above 100 data points, it was marked as \"High SS\". If it had between 50 and 100 data points, it was marked as \"Medium SS\", and less than 50 corresponds to \"Low SS\". If the regression plot had a median over 0.4, it is marked as \"High R2\". If it is between 0.4 and 0.1, it is marked as \"Medium R2\", and if lower than 0.1, it is marked as \"Low R2\". For source diversity, if the data set uses over two (2) different sources, it is \"High Source Diversity\". If there are only two (2) sources, it is \"Medium Source Diversity\", and if the data just comes from one (1) source then it is marked as \"Low Source Diversity\". “No clear sources” refers to cases where the source material was not marked for the component. For complete list of data sources utilized, see Data Sources tab within the file.\\n\\nAdditional Data Lifetime Cost Variation Considerations Data Sources Qualitative Rank Notes 14.8 Prevailing local wages... 1, 2, 3 Low SS, High R2, High Source Diversity Gas Cap as adder in Retail Price Intercepts', 'token_count': 444, 'element_types': ['table', 'listitem', 'title', 'narrativetext'], 'tables': ['<table><tr><td>Additional Data</td><td>Additional Data</td><td>Additional Data</td><td>Additional Data</td><td>Additional Data</td></tr><tr><td>Lifetime</td><td>Cost Variation Considerations</td><td>Data Sources</td><td>Qualitative Rank</td><td>Notes</td></tr><tr><td>14.8</td><td>Prevailing local wages...</td><td>1, 2, 3</td><td>Low SS, High R2, High Source Diversity</td><td>Gas Cap as adder in Retail Price Intercepts</td></tr></table>']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [2, 3], 'chunk_number': 3, 'text': 'البيانات الإضافية القسم الأخير من الملف يحتوي على بيانات إضافية غير مباشرة ضمن حساب كل مكون و فئة المنتج.\\n\\nتشمل هذه الحياة المتوقعة (في أعوام) للمكون الموجود والمواد المسائل الرطوبة الحالية حالة التدفق الموجودة الحاجة للتكثيف / التسريف الحاجة لإدخال في حالة الاحتراق من النظام الكهربائي الموجودة طبيعة المواد الخطيرة / حجم المواد المختلفة من التسريب، وقد يكون التأثيرات التالية تأثيرًا إضافيًا على تسعير كل مكون.\\n\\nتطبيق الأجور المحلية المهيمنة قياس الوقت الوصول / حالة / نوع من المعزلات الموجودة البناء والمواد الموجودة مسائل الرطوبة الموجودة الحاجة لإدخال في حالة الاحتراق من المواد الكهربائية الموجودة الطبيعة / حجم المواد المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة المعدة.', 'token_count': 444, 'element_types': ['الطاولة', 'القائمة', 'العنوان', 'النص الترويجي'], 'tables': ['البيانات الإضافية</td><td>بيانات إضافية</td><td>بيانات إضافية</td><td>بيانات إضافية</td><td>بيانات إضافية</td><td>بيانات إضافية</td><td><td>بيانات إضافية</td><td><td>بيانات إضافية</td><td>بيانات إضافية</td><td>بيانات إضافية</td><td>14.8</td><td><td>بيانات إضافية</td><td><td>بيانات إضافية</td><td><td>1، 2، 3</td><td><td>بيانات إضافية منخفضة، مرتفعة عالية، متنوعة عالية من المصدرات</td><td><td>قاومة أسعار الغاز كإضاف في خدمات التداول التجزئة</td></tr>']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [3], 'chunk_number': 4, 'text': 'Price Calculation Example', 'token_count': 3, 'element_types': ['title']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [3], 'chunk_number': 4, 'text': 'مثال على حساب الأسعار', 'token_count': 3, 'element_types': ['العنوان']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [3], 'chunk_number': 5, 'text': 'Example 1: Air Source Heat Pump (Retrofit Installation Multiplier) Example for calculating the low, mid, and high retail price along with the associated labor for replacing an air source heat pump that does not require a new circuit or panel upgrade. The numbers in red correspond to the different coefficients in the flat CSV file for the two performance metrics and the low, mid, and high regressions: Where A is the capacity in tons, B is the efficiency in SEER1, and C is the intercept value (constant). To produce the total installed cost, use the retrofit labor multiplier (if this was for new construction, the new construction multiplier would be used): Therefore, the median material price is $9,826 and the labor cost is $4,913 for a total installed cost of $20,635. The installation costs here include labor and equipment costs for demolition, removal, and installation. There are many reasons the price for a specific home could be higher or lower, some of which are mentioned in the Cost Variation Considerations column. Note in this example the labor cost is calculated by subtracting the material price from the installed cost.', 'token_count': 231, 'element_types': ['title', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [3], 'chunk_number': 5, 'text': 'مثال 1: مضخة حرارة مصدر الهواء (مضاعف تثبيت الترميم) مثال لحساب سعر التجزئة المنخفض والمتوسط والعالي جنبا إلى جنب مع العمالة المرتبطة باستبدال مضخة حرارة مصدر الهواء التي لا تتطلب تحديثًا جديدًا للدائرة أو اللوحة. تتوافق الأرقام باللون الأحمر مع المعاملات المختلفة في ملف CSV المسطح لمقاييس الأداءين والتراجعات المنخفضة والمتوسطة والعالية: حيث A هو القدرة في الأطنان ، و B هو الكفاءة في SEER1 ، و C هو قيمة التقاطع (مستمرة). لإنتاج إجمالي تكلفة التثبيت التثبيتية ، استخدم مضاع العمالة التثبيتية (إذا كان ذلك للبناء الجديد ، فسيتم استخدام مضاعل البناء الجديد): فإن متوسط سعر المواد هو 9.826 دولارًا وتكلفة العمالة هي 4.913 دولارًا لتكلفة المواد التثبيتة الإجمالية الإجمالية الإجمالية التي تبلغية 20.635 دولارًا. يمكن أن يحتسبق هنا قيمة العمل والتكلفة المعدل والتكلفة المعدل والتكلفة المعدل والتكلفة المحددة، والتكلفة المحددة، والتكلفة من تكلفة المعدل المعدل، والتكلفة المعدل المعدل، والتكلفة المعدل المعدل المعدل، والتكلفة المعدل، والتكلفة المعدل المعدل، والتكلفة المعدل المعدل.', 'token_count': 231, 'element_types': ['العنوان', 'النص الترويجي']}\n",
      "--------------------\n",
      "Original Chunk:\n",
      "{'source': 'Stats.docx', 'pages': [3], 'chunk_number': 6, 'text': 'Example 2 Unfinished Attic Ceiling Batt Insulation (Retrofit Installation Adder) Example for calculating the low, mid, and high retail price along with the associated labor for replacing (retrofitting) ceiling insulation in an unfinished attic with an R-Value of 15, using fiberglass batt insulation. The numbers in red correspond to the different coefficients in the Machine Readable CSV file for the performance metric and the low, mid, and high regressions: Where A is the R-value coefficient and C is the intercept value (constant). To produce the total installed cost, use the retrofit labor adder, (if this was for new construction, the new construction adder would be used): Therefore, the median material price is $0.86 per square foot and the labor cost is $1.00 per square foot for a total installed cost of $1.86 per square foot.', 'token_count': 181, 'element_types': ['title', 'narrativetext']}\n",
      "\n",
      "Translated Chunk:\n",
      "{'source': 'الإحصاءات.docx', 'pages': [3], 'chunk_number': 6, 'text': 'نموذج 2 عزل السقف غير المكتمل في الشرفة (مؤشر التثبيت) نموذج لحساب سعر التجزئة المنخفض والمتوسط والعالي جنبا إلى جنب مع العمل المرتبط لاستبدال عزل السقف في الشرفة غير المكتملة بقيمة R 15 ، باستخدام عزل الصيد الكلاسيكي. تتوافق الأرقام باللون الأحمر مع المعاملات المختلفة في ملف CSV القابلة للقراءة الآلية لمقاييس الأداء والتراجع المنخفض والمتوسط والعالي: حيث A هو معدل قيمة R و C هو القيمة المتقاطعة (مستمرة). لإنتاج إجمالي التكلفة المثبيتة ، استخدم معدل عمل التثبيت ، (إذا كان هذا للبناء الجديد ، فسيتم استخدام إعلان البناء الجديد): لذلك ، فإن متوسط سعر المواد هو $860.861.00 لكل قدم مربع وتكلفة العمل لكل قدم مربع من $861.00 لكل قدم مربع مثبت.', 'token_count': 181, 'element_types': ['العنوان', 'النص الترويجي']}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for original, translated in zip(grouped_chunks['Stats.docx'], translated_jsons):\n",
    "    print(\"Original Chunk:\")\n",
    "    print(original)\n",
    "    print(\"\\nTranslated Chunk:\")\n",
    "    print(translated)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l158Ymzrgx5i"
   },
   "source": [
    "#### **Translating entire file now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BXOQ4UeEHtHZ",
    "outputId": "e38dc9eb-9d57-46b1-8a0b-41a03d858234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 5 tokens processed in 0.67s (7.50 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 13.49 tokens/sec\n",
      "⏱️ Translation: 132 tokens processed in 10.60s (12.46 tokens/sec)\n",
      "   Total (in+out): 278 tokens at 26.24 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.57s (12.20 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 29.64 tokens/sec\n",
      "⏱️ Translation: 62 tokens processed in 3.38s (18.35 tokens/sec)\n",
      "   Total (in+out): 119 tokens at 35.21 tokens/sec\n",
      "⏱️ Translation: 249 tokens processed in 16.90s (14.73 tokens/sec)\n",
      "   Total (in+out): 520 tokens at 30.77 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.97s (8.23 tokens/sec)\n",
      "   Total (in+out): 19 tokens at 19.55 tokens/sec\n",
      "⏱️ Translation: 161 tokens processed in 12.06s (13.35 tokens/sec)\n",
      "   Total (in+out): 362 tokens at 30.02 tokens/sec\n",
      "⏱️ Translation: 86 tokens processed in 6.56s (13.11 tokens/sec)\n",
      "   Total (in+out): 206 tokens at 31.40 tokens/sec\n",
      "⏱️ Translation: 107 tokens processed in 10.41s (10.28 tokens/sec)\n",
      "   Total (in+out): 288 tokens at 27.67 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.56s (8.98 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 25.14 tokens/sec\n",
      "⏱️ Translation: 75 tokens processed in 6.06s (12.37 tokens/sec)\n",
      "   Total (in+out): 178 tokens at 29.37 tokens/sec\n",
      "⏱️ Translation: 26 tokens processed in 2.44s (10.66 tokens/sec)\n",
      "   Total (in+out): 68 tokens at 27.88 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.66s (12.04 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 25.58 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.75 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 24.38 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.46s (6.47 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 17.25 tokens/sec\n",
      "⏱️ Translation: 13 tokens processed in 0.79s (16.51 tokens/sec)\n",
      "   Total (in+out): 25 tokens at 31.74 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.69s (10.13 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 24.59 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.59s (11.85 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 27.10 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.48s (16.54 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 31.01 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.83s (13.18 tokens/sec)\n",
      "   Total (in+out): 26 tokens at 31.15 tokens/sec\n",
      "⏱️ Translation: 10 tokens processed in 0.67s (14.99 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 32.97 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.75s (11.99 tokens/sec)\n",
      "   Total (in+out): 21 tokens at 27.97 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.42s (21.32 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.54 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.81s (9.91 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 22.29 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.60s (11.61 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 24.88 tokens/sec\n",
      "⏱️ Translation: 260 tokens processed in 19.39s (13.41 tokens/sec)\n",
      "   Total (in+out): 461 tokens at 23.78 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.46s (15.33 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 30.65 tokens/sec\n",
      "⏱️ Translation: 22 tokens processed in 1.46s (15.07 tokens/sec)\n",
      "   Total (in+out): 48 tokens at 32.89 tokens/sec\n",
      "⏱️ Translation: 74 tokens processed in 6.85s (10.80 tokens/sec)\n",
      "   Total (in+out): 186 tokens at 27.15 tokens/sec\n",
      "⏱️ Translation: 30 tokens processed in 2.02s (14.88 tokens/sec)\n",
      "   Total (in+out): 67 tokens at 33.23 tokens/sec\n",
      "⏱️ Translation: 35 tokens processed in 2.80s (12.49 tokens/sec)\n",
      "   Total (in+out): 85 tokens at 30.33 tokens/sec\n",
      "⏱️ Translation: 108 tokens processed in 9.06s (11.92 tokens/sec)\n",
      "   Total (in+out): 263 tokens at 29.02 tokens/sec\n",
      "⏱️ Translation: 25 tokens processed in 1.81s (13.83 tokens/sec)\n",
      "   Total (in+out): 47 tokens at 26.00 tokens/sec\n",
      "⏱️ Translation: 92 tokens processed in 7.00s (13.14 tokens/sec)\n",
      "   Total (in+out): 208 tokens at 29.70 tokens/sec\n",
      "⏱️ Translation: 25 tokens processed in 1.41s (17.77 tokens/sec)\n",
      "   Total (in+out): 50 tokens at 35.54 tokens/sec\n",
      "⏱️ Translation: 36 tokens processed in 2.74s (13.15 tokens/sec)\n",
      "   Total (in+out): 86 tokens at 31.41 tokens/sec\n",
      "⏱️ Translation: 42 tokens processed in 3.07s (13.69 tokens/sec)\n",
      "   Total (in+out): 96 tokens at 31.30 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.65s (9.21 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 21.48 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.58s (13.80 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 27.61 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.68s (11.71 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 23.43 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.29s (13.75 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 27.50 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.32s (9.33 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 21.77 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.36 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 27.20 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.49s (10.22 tokens/sec)\n",
      "   Total (in+out): 13 tokens at 26.57 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.79s (11.43 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 27.95 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.27s (14.66 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 29.32 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.07s (13.14 tokens/sec)\n",
      "   Total (in+out): 31 tokens at 29.09 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.94s (18.00 tokens/sec)\n",
      "   Total (in+out): 34 tokens at 35.99 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.91s (18.65 tokens/sec)\n",
      "   Total (in+out): 33 tokens at 36.21 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.87s (18.39 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.78 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (18.00 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 35.99 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (17.93 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 35.86 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.04s (15.44 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 30.88 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.67s (11.93 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 25.34 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.66s (12.12 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 22.73 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.42s (18.96 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.55 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.34s (11.85 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 29.62 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.36s (8.35 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 19.48 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.65 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 30.35 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.26 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 24.53 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.35s (14.41 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 28.82 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.36s (13.79 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.59 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.36s (13.76 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.52 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.67s (5.93 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 14.82 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.32s (12.58 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 28.31 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.83 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.65 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.33s (11.94 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 23.88 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.07s (13.05 tokens/sec)\n",
      "   Total (in+out): 31 tokens at 28.89 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.95s (17.99 tokens/sec)\n",
      "   Total (in+out): 34 tokens at 35.97 tokens/sec\n",
      "⏱️ Translation: 17 tokens processed in 0.95s (17.96 tokens/sec)\n",
      "   Total (in+out): 33 tokens at 34.87 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.88s (18.11 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.23 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.13s (14.21 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 28.41 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 1.11s (14.46 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 28.93 tokens/sec\n",
      "⏱️ Translation: 16 tokens processed in 0.89s (18.01 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 36.02 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.53s (15.11 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 32.11 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.46s (17.24 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 32.32 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.42s (18.96 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 35.56 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (11.04 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.61 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.38s (7.99 tokens/sec)\n",
      "   Total (in+out): 7 tokens at 18.63 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.42s (12.01 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 28.83 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.48s (12.51 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 25.02 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.42s (9.47 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 18.94 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.30s (13.43 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 26.86 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.32s (12.53 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 25.06 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.47s (10.66 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 23.46 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (11.06 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 27.65 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.32s (9.33 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 18.66 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.82 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.65 tokens/sec\n",
      "⏱️ Translation: 12 tokens processed in 0.90s (13.33 tokens/sec)\n",
      "   Total (in+out): 23 tokens at 25.56 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.65s (16.94 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 33.88 tokens/sec\n",
      "⏱️ Translation: 11 tokens processed in 0.83s (13.22 tokens/sec)\n",
      "   Total (in+out): 22 tokens at 26.45 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.58s (10.37 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 20.75 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.16 tokens/sec)\n",
      "   Total (in+out): 11 tokens at 22.29 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.46s (13.04 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 30.42 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.37s (13.43 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 26.85 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.39s (12.73 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 25.46 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.42s (14.24 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 28.48 tokens/sec\n",
      "⏱️ Translation: 8 tokens processed in 0.52s (15.40 tokens/sec)\n",
      "   Total (in+out): 17 tokens at 32.73 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.59s (15.32 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 30.65 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.42s (11.81 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 23.61 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.54s (9.34 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 22.41 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.76 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 19.52 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.41s (9.70 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 24.25 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.52s (13.55 tokens/sec)\n",
      "   Total (in+out): 16 tokens at 30.96 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.51s (17.62 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 35.24 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.39s (12.70 tokens/sec)\n",
      "   Total (in+out): 10 tokens at 25.40 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.52s (9.65 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 23.17 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.31s (13.06 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 26.11 tokens/sec\n",
      "⏱️ Translation: 3 tokens processed in 0.31s (9.61 tokens/sec)\n",
      "   Total (in+out): 6 tokens at 19.21 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.53s (9.42 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 26.38 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.59s (15.34 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 30.69 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.74s (12.09 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 24.18 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.75s (11.97 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 23.94 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.70s (12.89 tokens/sec)\n",
      "   Total (in+out): 18 tokens at 25.77 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.29s (13.71 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 30.85 tokens/sec\n",
      "⏱️ Translation: 7 tokens processed in 0.58s (11.98 tokens/sec)\n",
      "   Total (in+out): 15 tokens at 25.67 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.40s (12.47 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 29.93 tokens/sec\n",
      "⏱️ Translation: 6 tokens processed in 0.49s (12.17 tokens/sec)\n",
      "   Total (in+out): 14 tokens at 28.39 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.30s (13.17 tokens/sec)\n",
      "   Total (in+out): 9 tokens at 29.63 tokens/sec\n",
      "⏱️ Translation: 4 tokens processed in 0.36s (10.97 tokens/sec)\n",
      "   Total (in+out): 8 tokens at 21.94 tokens/sec\n",
      "⏱️ Translation: 9 tokens processed in 0.67s (13.39 tokens/sec)\n",
      "   Total (in+out): 19 tokens at 28.27 tokens/sec\n",
      "⏱️ Translation: 5 tokens processed in 0.47s (10.55 tokens/sec)\n",
      "   Total (in+out): 12 tokens at 25.32 tokens/sec\n",
      "⏱️ Translation: 12 tokens processed in 0.99s (12.10 tokens/sec)\n",
      "   Total (in+out): 30 tokens at 30.26 tokens/sec\n",
      "⏱️ Translation: 14 tokens processed in 1.05s (13.31 tokens/sec)\n",
      "   Total (in+out): 32 tokens at 30.43 tokens/sec\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/translated/Stats_ar.docx'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate_file(file_path='/content/Stats.docx', target_language='ar', source_language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45WMA-kgifZr"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/UzairNaeem3/DrX_EnigmaticResearch/raw/master/images/Screenshot_8.png\" style=\"display: inline-block; width: 45%; margin: 1%;\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gmFHB8kjVJ-"
   },
   "source": [
    "**Not perfect, but better**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIuoyG8Ui_Ti"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"https://github.com/UzairNaeem3/DrX_EnigmaticResearch/raw/master/images/Screenshot_9.png\" style=\"display: inline-block; width: 45%; margin: 1%;\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDhYs9bch_Tc"
   },
   "source": [
    "### **Translating a PDF file now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lLxRBhTkLDXr",
    "outputId": "183c8744-ea55-4999-91a4-58d646b0d5fe"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting page-by-page translation of PDF: /content/The_Plan_of_the_Giza_Pyramids.pdf\n",
      "PDF has 16 pages\n",
      "Processing page 1 of 16\n",
      "⏱️ Translation: 643 tokens processed in 26.64s (24.14 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1091 tokens at 40.96 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 1 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_1.txt\n",
      "Processing page 2 of 16\n",
      "⏱️ Translation: 438 tokens processed in 18.10s (24.20 tokens/sec)\n",
      "\n",
      "   Total (in+out): 737 tokens at 40.72 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 2 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_2.txt\n",
      "Processing page 3 of 16\n",
      "⏱️ Translation: 697 tokens processed in 37.14s (18.77 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1267 tokens at 34.11 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 3 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_3.txt\n",
      "Processing page 4 of 16\n",
      "⏱️ Translation: 803 tokens processed in 33.95s (23.65 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1240 tokens at 36.52 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 4 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_4.txt\n",
      "Processing page 5 of 16\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Translation: 980 tokens processed in 111.31s (8.80 tokens/sec)\n",
      "\n",
      "   Total (in+out): 2182 tokens at 19.60 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 5 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_5.txt\n",
      "Processing page 6 of 16\n",
      "⏱️ Translation: 819 tokens processed in 74.48s (11.00 tokens/sec)\n",
      "\n",
      "   Total (in+out): 2137 tokens at 28.69 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 6 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_6.txt\n",
      "Processing page 7 of 16\n",
      "⏱️ Translation: 463 tokens processed in 19.44s (23.82 tokens/sec)\n",
      "\n",
      "   Total (in+out): 790 tokens at 40.64 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 7 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_7.txt\n",
      "Processing page 8 of 16\n",
      "⏱️ Translation: 333 tokens processed in 55.91s (5.96 tokens/sec)\n",
      "\n",
      "   Total (in+out): 614 tokens at 10.98 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 8 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_8.txt\n",
      "Processing page 9 of 16\n",
      "⏱️ Translation: 477 tokens processed in 56.09s (8.50 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1454 tokens at 25.92 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 9 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_9.txt\n",
      "Processing page 10 of 16\n",
      "⏱️ Translation: 337 tokens processed in 42.30s (7.97 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1109 tokens at 26.22 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 10 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_10.txt\n",
      "Processing page 11 of 16\n",
      "⏱️ Translation: 689 tokens processed in 62.45s (11.03 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1768 tokens at 28.31 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 11 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_11.txt\n",
      "Processing page 12 of 16\n",
      "⏱️ Translation: 359 tokens processed in 55.73s (6.44 tokens/sec)\n",
      "\n",
      "   Total (in+out): 611 tokens at 10.96 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 12 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_12.txt\n",
      "Processing page 13 of 16\n",
      "⏱️ Translation: 274 tokens processed in 15.66s (17.50 tokens/sec)\n",
      "\n",
      "   Total (in+out): 511 tokens at 32.63 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 13 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_13.txt\n",
      "Processing page 14 of 16\n",
      "⏱️ Translation: 408 tokens processed in 16.92s (24.11 tokens/sec)\n",
      "\n",
      "   Total (in+out): 680 tokens at 40.19 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 14 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_14.txt\n",
      "Processing page 15 of 16\n",
      "⏱️ Translation: 765 tokens processed in 37.53s (20.38 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1330 tokens at 35.44 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 15 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_15.txt\n",
      "Processing page 16 of 16\n",
      "⏱️ Translation: 948 tokens processed in 44.64s (21.24 tokens/sec)\n",
      "\n",
      "   Total (in+out): 1608 tokens at 36.02 tokens/sec\n",
      "\n",
      "\n",
      "Saved translated page 16 to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages/page_16.txt\n",
      "Saved complete translated text to /content/translated/The_Plan_of_the_Giza_Pyramids_ar.txt\n",
      "Individual translated pages saved to /content/translated/The_Plan_of_the_Giza_Pyramids_ar_pages\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/translated/The_Plan_of_the_Giza_Pyramids_ar.txt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate_file(file_path='/content/The_Plan_of_the_Giza_Pyramids.pdf', target_language='ar', source_language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4gN4AEpsBRb"
   },
   "source": [
    "\n",
    "\n",
    "> It does translate the PDF, but after translation, it wasn't converting back into a PDF correctly. So, I saved the translated text as a .txt file instead of a PDF. However, when dealing with complex PDFs that contain tables, the results were still not satisfactory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7vuNcVwlGSy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
