{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2baKbzQt8wxb"
      },
      "outputs": [],
      "source": [
        "!apt-get install poppler-utils -q\n",
        "!apt-get install tesseract-ocr-all\n",
        "!pip install tiktoken -q\n",
        "!pip install unstructured -q\n",
        "!pip install unstructured['pdf'] -q\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9ZRbNIg9m5D",
        "outputId": "79f32ad4-3552-4834-9472-abe1acf8c845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/files\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "files_path = '/content/Dr.X Files.zip'\n",
        "extract_path = '/content/files'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract all files from the zip archive\n",
        "with zipfile.ZipFile(files_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5gpvu-ymNpx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import tiktoken\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Set\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.partition.docx import partition_docx\n",
        "from unstructured.partition.xlsx import partition_xlsx\n",
        "from unstructured.partition.csv import partition_csv\n",
        "from unstructured.staging.base import elements_to_json\n",
        "from unstructured.cleaners.core import clean_extra_whitespace, replace_unicode_quotes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Chunking Strategy:\n",
        "\n",
        "- **Semantic Chunking**: Creates chunks based on natural document boundaries like `headings` and `sections` rather than arbitrary splits\n",
        "- **Size-Based Chunking**: Ensures chunks stay within a specified token limit `default 512 tokens`\n",
        "- **Overlap Mechanism**: Maintains context between chunks by including overlap text `default 50 tokens`\n",
        "- **Element Type Awareness**: Preserves structure by handling different content types `text, tables, images` appropriately\n",
        "- **Boundary Detection**: Automatically identifies `semantic boundaries` at headings, titles, and section markers\n",
        "- **Table Handling**: Special handling for tables, keeping them intact when possible or creating dedicated chunks\n",
        "- **Metadata Preservation**: Each chunk retains source information, page numbers, and element types\n",
        "- **Empty Element Filtering**: Skips empty elements of certain types that don't contribute to content."
      ],
      "metadata": {
        "id": "Mp3LUEqRrWk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedDocumentChunker:\n",
        "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50,\n",
        "                 tokenizer_name: str = \"cl100k_base\",\n",
        "                 skip_if_empty: List[str] = None,\n",
        "                 ):\n",
        "\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.tokenizer_name = tokenizer_name\n",
        "        self.tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
        "        self.skip_if_empty = skip_if_empty or [\"image\", \"figure\"]\n",
        "\n",
        "    def process_document(self, file_path: str, include_page_breaks: bool = True,\n",
        "                         strategy: str = \"hi_res\", infer_table_structure: bool = True,\n",
        "                         hi_res_model_name: str = None) -> List[Dict]:\n",
        "\n",
        "        \"\"\"Parses and processes a document (PDF, DOCX, XLSX, or CSV) into a list of structured elements.\"\"\"\n",
        "\n",
        "        print(f\"Processing: {file_path}\")\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "        try:\n",
        "            if file_extension == '.pdf':\n",
        "                elements = partition_pdf(\n",
        "                    filename=file_path,\n",
        "                    include_page_breaks=include_page_breaks,\n",
        "                    include_metadata=True,\n",
        "                    strategy=strategy,\n",
        "                    infer_table_structure=infer_table_structure,\n",
        "                    hi_res_model_name=hi_res_model_name,\n",
        "                )\n",
        "            elif file_extension in ['.docx', '.doc']:\n",
        "                elements = partition_docx(\n",
        "                    filename=file_path,\n",
        "                    include_page_breaks=include_page_breaks,\n",
        "                    include_metadata=True,\n",
        "                    strategy=strategy,\n",
        "                    infer_table_structure=infer_table_structure,\n",
        "                    hi_res_model_name=hi_res_model_name,\n",
        "                )\n",
        "            elif file_extension in ['.xlsx', 'xls']:\n",
        "                elements = partition_xlsx(\n",
        "                    filename=file_path,\n",
        "                    include_metadata=True,\n",
        "                )\n",
        "            elif file_extension == '.csv':\n",
        "                elements = partition_csv(\n",
        "                    filename=file_path,\n",
        "                    include_metadata=True\n",
        "                    )\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
        "\n",
        "            # Convert to dictionaries which are easier to work with\n",
        "            elements_json_str = elements_to_json(elements)\n",
        "            element_dicts = json.loads(elements_json_str)\n",
        "\n",
        "            # Clean and normalize text\n",
        "            for element in element_dicts:\n",
        "                if \"text\" in element:\n",
        "                    element[\"text\"] = self._clean_text(element[\"text\"])\n",
        "\n",
        "            # Filter out empty elements of certain types\n",
        "            filtered_elements = []\n",
        "            for element in element_dicts:\n",
        "                element_type = self._get_element_type(element)\n",
        "                element_text = self._get_element_text(element)\n",
        "\n",
        "                # Skip empty elements of specific types that don't contribute to content\n",
        "                if element_type in self.skip_if_empty and not element_text.strip():\n",
        "                    continue\n",
        "\n",
        "                filtered_elements.append(element)\n",
        "            return filtered_elements\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def process_directory(self, directory_path: str, extensions: List[str] = None,\n",
        "                          include_page_breaks: bool = True, strategy: str = \"hi_res\",\n",
        "                          hi_res_model_name: str = None,\n",
        "                          infer_table_structure: bool = True) -> Dict[str, List[Dict]]:\n",
        "\n",
        "        \"\"\"Processes all supported documents in a directory and saves extracted content as JSON files.\"\"\"\n",
        "\n",
        "        # Set default extensions if none are provided\n",
        "        if extensions is None:\n",
        "            extensions = ['.pdf', '.docx', '.doc']\n",
        "\n",
        "        all_documents = {}\n",
        "        # Create directory to store processed JSON files\n",
        "        output_dir = os.path.join(os.getcwd(), \"processed_json\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Iterate through all files in the provided directory\n",
        "        for filename in os.listdir(directory_path):\n",
        "            file_ext = os.path.splitext(filename)[1].lower()\n",
        "\n",
        "            # Only process files with the allowed extensions\n",
        "            if file_ext in extensions:\n",
        "                try:\n",
        "                    file_path = os.path.join(directory_path, filename)\n",
        "\n",
        "                    # Process individual document\n",
        "                    elements = self.process_document(\n",
        "                        file_path,\n",
        "                        include_page_breaks=include_page_breaks,\n",
        "                        strategy=strategy,\n",
        "                        hi_res_model_name=hi_res_model_name,\n",
        "                        infer_table_structure=infer_table_structure\n",
        "                    )\n",
        "\n",
        "                    if elements:\n",
        "                        # Store the results in the dictionary\n",
        "                        all_documents[filename] = elements\n",
        "                        print(f\"Extracted {len(elements)} elements from {filename}\")\n",
        "\n",
        "                        # Save each processed document to a separate JSON file\n",
        "                        base_name = os.path.splitext(filename)[0]\n",
        "                        output_file_path = os.path.join(output_dir, f\"{base_name}.json\")\n",
        "                        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "                            json.dump(elements, f, indent=2, ensure_ascii=False)\n",
        "                        print(f\"Saved JSON for {filename} to {output_file_path}\")\n",
        "                    else:\n",
        "                        print(f\"No elements extracted from {filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {str(e)}\")\n",
        "\n",
        "        return all_documents\n",
        "\n",
        "    def create_semantic_chunks(self, elements: List[Dict[Any, Any]], source: str = \"\") -> List[Dict[Any, Any]]:\n",
        "        \"\"\"Converts parsed document elements into semantically meaningful text chunks with metadata.\"\"\"\n",
        "\n",
        "        if not elements:\n",
        "            return []\n",
        "\n",
        "        chunks = []\n",
        "        chunk_count = 0\n",
        "\n",
        "        # Initialize processing state\n",
        "        current_chunk_text = \"\"\n",
        "        current_chunk_tokens = []\n",
        "        current_chunk_pages = set()\n",
        "        current_element_types = set()\n",
        "        current_tables = []\n",
        "\n",
        "        # Helper function to finalize and store the current chunk\n",
        "        def add_chunk():\n",
        "            nonlocal chunk_count, current_chunk_text, current_chunk_tokens, current_chunk_pages\n",
        "            nonlocal current_element_types, current_tables\n",
        "\n",
        "            if not current_chunk_text.strip():\n",
        "                return\n",
        "\n",
        "            chunk = {\n",
        "                \"source\": source,\n",
        "                \"pages\": sorted(list(current_chunk_pages)),\n",
        "                \"chunk_number\": chunk_count,\n",
        "                \"text\": current_chunk_text,\n",
        "                \"token_count\": len(current_chunk_tokens),\n",
        "                \"element_types\": list(current_element_types)\n",
        "            }\n",
        "\n",
        "            if current_tables:\n",
        "                chunk[\"tables\"] = current_tables\n",
        "\n",
        "            chunks.append(chunk)\n",
        "            chunk_count += 1\n",
        "\n",
        "        # Iterate through elements and group them into semantically meaningful chunks\n",
        "        current_page = None\n",
        "        i = 0\n",
        "        while i < len(elements):\n",
        "            element = elements[i]\n",
        "            element_text = self._get_element_text(element)\n",
        "            element_type = self._get_element_type(element)\n",
        "            page = self._get_element_page_number(element)\n",
        "\n",
        "            # Skip empty elements\n",
        "            if not element_text and not element.get(\"metadata\", {}).get(\"text_as_html\"):\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # Track page changes\n",
        "            if page is not None and page != current_page:\n",
        "                current_page = page\n",
        "\n",
        "            # Handle table elements with HTML metadata\n",
        "            if element_type == \"table\" and element.get(\"metadata\", {}).get(\"text_as_html\"):\n",
        "                table_html = element[\"metadata\"][\"text_as_html\"]\n",
        "                element_tokens = self.tokenizer.encode(element_text)\n",
        "\n",
        "                # If adding this table would exceed chunk size\n",
        "                if current_chunk_text and len(current_chunk_tokens) + len(element_tokens) > self.chunk_size:\n",
        "                    # Save current chunk before starting new one with the table\n",
        "                    add_chunk()\n",
        "\n",
        "                    # Start a new chunk with the table\n",
        "                    current_chunk_text = element_text\n",
        "                    current_chunk_tokens = element_tokens\n",
        "                    current_chunk_pages = {page} if page is not None else set()\n",
        "                    current_element_types = {element_type}\n",
        "                    current_tables = [table_html]\n",
        "                else:\n",
        "                    # Add table to current chunk\n",
        "                    if current_chunk_text:\n",
        "                        current_chunk_text += \"\\n\\n\" + element_text\n",
        "                    else:\n",
        "                        current_chunk_text = element_text\n",
        "                    current_chunk_tokens.extend(element_tokens)\n",
        "                    if page is not None:\n",
        "                        current_chunk_pages.add(page)\n",
        "                    current_element_types.add(element_type)\n",
        "                    current_tables.append(table_html)\n",
        "\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # If a semantic boundary is detected and chunk has content, save it\n",
        "            if self._is_semantic_boundary(element) and current_chunk_text:\n",
        "                add_chunk()\n",
        "\n",
        "                # Reset for new chunk\n",
        "                current_chunk_text = \"\"\n",
        "                current_chunk_tokens = []\n",
        "                current_chunk_pages = set()\n",
        "                current_element_types = set()\n",
        "                current_tables = []\n",
        "\n",
        "            # Process normal elements\n",
        "            element_tokens = self.tokenizer.encode(element_text)\n",
        "\n",
        "            # If current chunk would exceed limit, save and start a new one (with optional overlap)\n",
        "            if len(current_chunk_tokens) + len(element_tokens) > self.chunk_size and current_chunk_text:\n",
        "                # Save current chunk\n",
        "                add_chunk()\n",
        "\n",
        "                # Start new chunk with overlap if needed\n",
        "                if self.chunk_overlap > 0 and len(current_chunk_tokens) > self.chunk_overlap:\n",
        "                    overlap_start = max(0, len(current_chunk_tokens) - self.chunk_overlap)\n",
        "                    overlap_text = self.tokenizer.decode(current_chunk_tokens[overlap_start:])\n",
        "\n",
        "                    # Reset with overlap text\n",
        "                    current_chunk_text = overlap_text\n",
        "                    current_chunk_tokens = current_chunk_tokens[overlap_start:]\n",
        "                    # Keep current page for overlap\n",
        "                    if page is not None:\n",
        "                        current_chunk_pages = {page}\n",
        "                    else:\n",
        "                        current_chunk_pages = set()\n",
        "                    # Keep element types\n",
        "                    # But reset tables - they're difficult to split with overlap\n",
        "                    current_tables = []\n",
        "                else:\n",
        "                    # Reset completely\n",
        "                    current_chunk_text = \"\"\n",
        "                    current_chunk_tokens = []\n",
        "                    current_chunk_pages = set()\n",
        "                    current_element_types = set()\n",
        "                    current_tables = []\n",
        "\n",
        "            # Add current element to the chunk\n",
        "            if current_chunk_text:\n",
        "                current_chunk_text += \" \" + element_text\n",
        "            else:\n",
        "                current_chunk_text = element_text\n",
        "\n",
        "            current_chunk_tokens.extend(element_tokens)\n",
        "            if page is not None:\n",
        "                current_chunk_pages.add(page)\n",
        "            current_element_types.add(element_type)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        # Add the final chunk if there's any content left\n",
        "        if current_chunk_text:\n",
        "            add_chunk()\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_and_chunk_document(self, file_path: str) -> List[Dict[Any, Any]]:\n",
        "        \"\"\"Process a document and create semantic chunks in one step.\"\"\"\n",
        "\n",
        "        elements = self.process_document(file_path)\n",
        "        if not elements:\n",
        "            return []\n",
        "\n",
        "        source = os.path.basename(file_path)\n",
        "        return self.create_semantic_chunks(elements, source)\n",
        "\n",
        "    def process_and_chunk_documents(self, documents: Dict[str, List[Dict]]) -> List[Dict[Any, Any]]:\n",
        "        \"\"\"Process all documents and create semantic chunks.\"\"\"\n",
        "\n",
        "        all_chunks = []\n",
        "        for filename, elements in documents.items():\n",
        "            source = filename\n",
        "            document_chunks = self.create_semantic_chunks(elements, source)\n",
        "            all_chunks.extend(document_chunks)\n",
        "            print(f\"Created {len(document_chunks)} chunks for {filename}\")\n",
        "\n",
        "        return all_chunks\n",
        "\n",
        "    def save_chunks(self, chunks: List[Dict[Any, Any]], output_dir: str = \"chunked_data\") -> None:\n",
        "        \"\"\"Save chunks to disk in JSON format.\"\"\"\n",
        "\n",
        "        if not chunks:\n",
        "            print(\"No chunks to save.\")\n",
        "            return\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Group chunks by source\n",
        "        chunks_by_source = {}\n",
        "        for chunk in chunks:\n",
        "            source = chunk.get(\"source\")\n",
        "            if source not in chunks_by_source:\n",
        "                chunks_by_source[source] = []\n",
        "            chunks_by_source[source].append(chunk)\n",
        "\n",
        "        # Save individual source files\n",
        "        for source, source_chunks in chunks_by_source.items():\n",
        "            base_name = os.path.splitext(source)[0]\n",
        "            output_file = os.path.join(output_dir, f\"{base_name}_chunks.json\")\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(source_chunks, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"Saved {len(source_chunks)} chunks for {source}\")\n",
        "\n",
        "        # Save all chunks to a single file\n",
        "        all_chunks_file = os.path.join(output_dir, \"all_chunks.json\")\n",
        "        with open(all_chunks_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Saved {len(chunks)} total chunks to {output_dir}/all_chunks.json\")\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text content.\"\"\"\n",
        "\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Clean extra whitespace\n",
        "        text = clean_extra_whitespace(text)\n",
        "        # Replace Unicode quotes\n",
        "        text = replace_unicode_quotes(text)\n",
        "\n",
        "        # Additional cleaning if needed\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _get_element_text(self, element: Dict) -> str:\n",
        "        \"\"\"Extract text from an element with formatting cleanup.\"\"\"\n",
        "\n",
        "        return element.get(\"text\", \"\").strip()\n",
        "\n",
        "    def _get_element_type(self, element: Dict) -> str:\n",
        "        \"\"\"Get the type of an element for structural chunking.\"\"\"\n",
        "\n",
        "        return element.get(\"type\", \"Unknown\").lower()\n",
        "\n",
        "    def _get_element_metadata(self, element: Dict) -> Dict:\n",
        "        \"\"\"Extract metadata from an element dict.\"\"\"\n",
        "\n",
        "        return element.get(\"metadata\", {})\n",
        "\n",
        "    def _get_element_page_number(self, element: Dict) -> Optional[int]:\n",
        "        \"\"\"Extract page number from element metadata\"\"\"\n",
        "\n",
        "        metadata = self._get_element_metadata(element)\n",
        "        return metadata.get(\"page_number\")\n",
        "\n",
        "    def _is_semantic_boundary(self, element: Dict) -> bool:\n",
        "        \"\"\"Check if an element is a semantic boundary.\"\"\"\n",
        "\n",
        "        element_type = self._get_element_type(element)\n",
        "        text = element.get(\"text\", \"\").strip()\n",
        "\n",
        "        # Check for headings, titles, section markers\n",
        "        if element_type in [\"heading\", \"title\", \"section\", \"subsection\", \"header\"]:\n",
        "            return True\n",
        "\n",
        "        # Check for page-break that coincides with semantic boundaries\n",
        "        if \"page_break\" in element_type and text:\n",
        "            return True\n",
        "\n",
        "        return False"
      ],
      "metadata": {
        "id": "kcxoxBENoEjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the chunker\n",
        "chunker = EnhancedDocumentChunker(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=50,\n",
        "    tokenizer_name=\"cl100k_base\",\n",
        "    skip_if_empty=[\"Image\", \"Figure\"],\n",
        ")\n",
        "\n",
        "# Process the documents\n",
        "documents = chunker.process_directory(\"/content/files\")\n",
        "\n",
        "# Create chunks\n",
        "chunks = chunker.process_and_chunk_documents(documents)\n",
        "\n",
        "# Save chunks\n",
        "chunker.save_chunks(chunks)\n",
        "print(f\"Chunking complete. Created {len(chunks)} total chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgeI1Ba9oJrD",
        "outputId": "40a97d25-d8e7-40a9-a998-ba3736477f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: /content/files/new-approaches-and-procedures-for-cancer-treatment.pdf\n",
            "Extracted 234 elements from new-approaches-and-procedures-for-cancer-treatment.pdf\n",
            "Saved JSON for new-approaches-and-procedures-for-cancer-treatment.pdf to /content/processed_json/new-approaches-and-procedures-for-cancer-treatment.json\n",
            "Processing: /content/files/The_Plan_of_the_Giza_Pyramids.pdf\n",
            "Extracted 218 elements from The_Plan_of_the_Giza_Pyramids.pdf\n",
            "Saved JSON for The_Plan_of_the_Giza_Pyramids.pdf to /content/processed_json/The_Plan_of_the_Giza_Pyramids.json\n",
            "Processing: /content/files/The-Alchemist.pdf\n",
            "Extracted 1435 elements from The-Alchemist.pdf\n",
            "Saved JSON for The-Alchemist.pdf to /content/processed_json/The-Alchemist.json\n",
            "Processing: /content/files/Stats.docx\n",
            "Extracted 46 elements from Stats.docx\n",
            "Saved JSON for Stats.docx to /content/processed_json/Stats.json\n",
            "Processing: /content/files/Ocean_ecogeochemistry_A_review.pdf\n",
            "Extracted 1235 elements from Ocean_ecogeochemistry_A_review.pdf\n",
            "Saved JSON for Ocean_ecogeochemistry_A_review.pdf to /content/processed_json/Ocean_ecogeochemistry_A_review.json\n",
            "Processing: /content/files/Dataset summaries and citations.docx\n",
            "Extracted 28 elements from Dataset summaries and citations.docx\n",
            "Saved JSON for Dataset summaries and citations.docx to /content/processed_json/Dataset summaries and citations.json\n",
            "Processing: /content/files/M.Sc. Applied Psychology.docx\n",
            "Extracted 1324 elements from M.Sc. Applied Psychology.docx\n",
            "Saved JSON for M.Sc. Applied Psychology.docx to /content/processed_json/M.Sc. Applied Psychology.json\n",
            "Created 57 chunks for new-approaches-and-procedures-for-cancer-treatment.pdf\n",
            "Created 34 chunks for The_Plan_of_the_Giza_Pyramids.pdf\n",
            "Created 150 chunks for The-Alchemist.pdf\n",
            "Created 7 chunks for Stats.docx\n",
            "Created 283 chunks for Ocean_ecogeochemistry_A_review.pdf\n",
            "Created 7 chunks for Dataset summaries and citations.docx\n",
            "Created 61 chunks for M.Sc. Applied Psychology.docx\n",
            "Saved 57 chunks for new-approaches-and-procedures-for-cancer-treatment.pdf\n",
            "Saved 34 chunks for The_Plan_of_the_Giza_Pyramids.pdf\n",
            "Saved 150 chunks for The-Alchemist.pdf\n",
            "Saved 7 chunks for Stats.docx\n",
            "Saved 283 chunks for Ocean_ecogeochemistry_A_review.pdf\n",
            "Saved 7 chunks for Dataset summaries and citations.docx\n",
            "Saved 61 chunks for M.Sc. Applied Psychology.docx\n",
            "Saved 599 total chunks to chunked_data/all_chunks.json\n",
            "Chunking complete. Created 599 total chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zipping the directories to use them later without processing again\n",
        "!zip -r /content/chunked_data.zip /content/chunked_data\n",
        "!zip -r /content/processed_json.zip /content/processed_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toMQv461urUI",
        "outputId": "14d30613-8da4-4a3b-f8a3-7a3b6530cc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/chunked_data/ (stored 0%)\n",
            "  adding: content/chunked_data/The-Alchemist_chunks.json (deflated 71%)\n",
            "  adding: content/chunked_data/Ocean_ecogeochemistry_A_review_chunks.json (deflated 74%)\n",
            "  adding: content/chunked_data/Stats_chunks.json (deflated 72%)\n",
            "  adding: content/chunked_data/new-approaches-and-procedures-for-cancer-treatment_chunks.json (deflated 68%)\n",
            "  adding: content/chunked_data/all_chunks.json (deflated 73%)\n",
            "  adding: content/chunked_data/The_Plan_of_the_Giza_Pyramids_chunks.json (deflated 72%)\n",
            "  adding: content/chunked_data/M.Sc. Applied Psychology_chunks.json (deflated 76%)\n",
            "  adding: content/chunked_data/Dataset summaries and citations_chunks.json (deflated 84%)\n",
            "  adding: content/processed_json/ (stored 0%)\n",
            "  adding: content/processed_json/Dataset summaries and citations.json (deflated 86%)\n",
            "  adding: content/processed_json/M.Sc. Applied Psychology.json (deflated 89%)\n",
            "  adding: content/processed_json/The-Alchemist.json (deflated 86%)\n",
            "  adding: content/processed_json/Ocean_ecogeochemistry_A_review.json (deflated 88%)\n",
            "  adding: content/processed_json/Stats.json (deflated 86%)\n",
            "  adding: content/processed_json/new-approaches-and-procedures-for-cancer-treatment.json (deflated 85%)\n",
            "  adding: content/processed_json/The_Plan_of_the_Giza_Pyramids.json (deflated 87%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-be4QcPH3jL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}